{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nmslib\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from lang_model_utils import load_lm_vocab, Query2Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lang_model_utils import load_lm_vocab, Query2Emb\n",
    "# from general_utils import create_nmslib_search_index\n",
    "\n",
    "input_path = Path('./data/stackoverflow/processed_data/')\n",
    "# code2emb_path = Path('./data/code2emb/')\n",
    "output_path = Path('./data/stackoverflow/search')\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank results with similarity, votes, and comments semtiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_df = pd.read_csv(input_path/'test.content_token', header=None, names=['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read in votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file of votes\n",
    "vote_df = pd.read_csv(input_path/'test.vote', header=None, names=['vote'])\n",
    "assert body_df.shape[0] == vote_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file of comments\n",
    "comment_df = pd.read_csv(input_path/'test.comment')\n",
    "assert body_df.shape[0] == comment_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file of urls\n",
    "url_df = pd.read_csv(input_path/'test.url', header=None, names=['url'])\n",
    "assert body_df.shape[0] == url_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://stackoverflow.com/a/18622422</td>\n",
       "      <td>first , create a list containing one hundred d...</td>\n",
       "      <td>Hi Kevin, I tried the latter code and it worke...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.292424</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://stackoverflow.com/a/5185748</td>\n",
       "      <td>i 've had a similar problem and i 've ended up...</td>\n",
       "      <td>IMO Fabio's solution is quite to the point :)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://stackoverflow.com/a/59841</td>\n",
       "      <td>two options that do n't require copying the wh...</td>\n",
       "      <td>`next(iter(your_list or []), None)` to handle ...</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://stackoverflow.com/a/2659296</td>\n",
       "      <td>i do n't think you can do this in one database...</td>\n",
       "      <td>Thanks. Note that in the example you gave, the...</td>\n",
       "      <td>0.167308</td>\n",
       "      <td>0.442949</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://stackoverflow.com/a/26097790</td>\n",
       "      <td>for python 3 :</td>\n",
       "      <td>@MartijnPieters Do you know what version intro...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    url  \\\n",
       "0  https://stackoverflow.com/a/18622422   \n",
       "1   https://stackoverflow.com/a/5185748   \n",
       "2     https://stackoverflow.com/a/59841   \n",
       "3   https://stackoverflow.com/a/2659296   \n",
       "4  https://stackoverflow.com/a/26097790   \n",
       "\n",
       "                                             content  \\\n",
       "0  first , create a list containing one hundred d...   \n",
       "1  i 've had a similar problem and i 've ended up...   \n",
       "2  two options that do n't require copying the wh...   \n",
       "3  i do n't think you can do this in one database...   \n",
       "4                                     for python 3 :   \n",
       "\n",
       "                                             comment  sentiment_polarity  \\\n",
       "0  Hi Kevin, I tried the latter code and it worke...            0.090909   \n",
       "1      IMO Fabio's solution is quite to the point :)            0.500000   \n",
       "2  `next(iter(your_list or []), None)` to handle ...            0.037500   \n",
       "3  Thanks. Note that in the example you gave, the...            0.167308   \n",
       "4  @MartijnPieters Do you know what version intro...           -0.100000   \n",
       "\n",
       "   sentiment_subjectivity  vote  \n",
       "0                0.292424     4  \n",
       "1                1.000000     1  \n",
       "2                0.570833   371  \n",
       "3                0.442949     7  \n",
       "4                0.275000     4  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect these two together into a dataframe\n",
    "ref_df = pd.concat([url_df, body_df, comment_df, vote_df], axis = 1).reset_index(drop=True)\n",
    "ref_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tanh to process votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://stackoverflow.com/a/18622422</td>\n",
       "      <td>first , create a list containing one hundred d...</td>\n",
       "      <td>Hi Kevin, I tried the latter code and it worke...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.292424</td>\n",
       "      <td>0.999329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://stackoverflow.com/a/5185748</td>\n",
       "      <td>i 've had a similar problem and i 've ended up...</td>\n",
       "      <td>IMO Fabio's solution is quite to the point :)</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://stackoverflow.com/a/59841</td>\n",
       "      <td>two options that do n't require copying the wh...</td>\n",
       "      <td>`next(iter(your_list or []), None)` to handle ...</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://stackoverflow.com/a/2659296</td>\n",
       "      <td>i do n't think you can do this in one database...</td>\n",
       "      <td>Thanks. Note that in the example you gave, the...</td>\n",
       "      <td>0.583654</td>\n",
       "      <td>0.442949</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://stackoverflow.com/a/26097790</td>\n",
       "      <td>for python 3 :</td>\n",
       "      <td>@MartijnPieters Do you know what version intro...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.999329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    url  \\\n",
       "0  https://stackoverflow.com/a/18622422   \n",
       "1   https://stackoverflow.com/a/5185748   \n",
       "2     https://stackoverflow.com/a/59841   \n",
       "3   https://stackoverflow.com/a/2659296   \n",
       "4  https://stackoverflow.com/a/26097790   \n",
       "\n",
       "                                             content  \\\n",
       "0  first , create a list containing one hundred d...   \n",
       "1  i 've had a similar problem and i 've ended up...   \n",
       "2  two options that do n't require copying the wh...   \n",
       "3  i do n't think you can do this in one database...   \n",
       "4                                     for python 3 :   \n",
       "\n",
       "                                             comment  sentiment_polarity  \\\n",
       "0  Hi Kevin, I tried the latter code and it worke...            0.545455   \n",
       "1      IMO Fabio's solution is quite to the point :)            0.750000   \n",
       "2  `next(iter(your_list or []), None)` to handle ...            0.518750   \n",
       "3  Thanks. Note that in the example you gave, the...            0.583654   \n",
       "4  @MartijnPieters Do you know what version intro...            0.450000   \n",
       "\n",
       "   sentiment_subjectivity      vote  \n",
       "0                0.292424  0.999329  \n",
       "1                1.000000  0.761594  \n",
       "2                0.570833  1.000000  \n",
       "3                0.442949  0.999998  \n",
       "4                0.275000  0.999329  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_df.vote = np.tanh(ref_df.vote)\n",
    "ref_df[['sentiment_polarity', 'sentiment_subjectivity']] = preprocessing.MinMaxScaler().fit_transform(ref_df[['sentiment_polarity', 'sentiment_subjectivity']])\n",
    "ref_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve top 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw contents\n",
    "with open(input_path/'train.content_token', 'r') as f:\n",
    "    trn_raw = f.readlines()\n",
    "\n",
    "with open(input_path/'valid.content_token', 'r') as f:\n",
    "    val_raw = f.readlines()\n",
    "    \n",
    "with open(input_path/'test.content_token', 'r') as f:\n",
    "    test_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loaded vocab of size 27,877\n"
     ]
    }
   ],
   "source": [
    "# load vocab \n",
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load language model\n",
    "lang_model = torch.load('./data/stackoverflow/lang_model/lang_model_cpu.torch', \n",
    "                        map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1 rows\n"
     ]
    }
   ],
   "source": [
    "q2emb = Query2Emb(lang_model = lang_model.cpu(),\n",
    "                  vocab = vocab)\n",
    "\n",
    "search_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "\n",
    "# use pre build index \n",
    "#TODO: rebuild index for bigger data set\n",
    "search_index.loadIndex('./data/stackoverflow/lang_model_emb/dim500_avg_searchindex.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test = q2emb.emb_mean('Hello World!  This is a test.')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_engine:\n",
    "    \"\"\"Organizes all the necessary elements we need to make a search engine.\"\"\"\n",
    "    def __init__(self, \n",
    "                 nmslib_index, \n",
    "                 ref_df, \n",
    "                 query2emb_func):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ==========\n",
    "        nmslib_index : nmslib object\n",
    "            This is pre-computed search index.\n",
    "        ref_df : pandas.DataFrame\n",
    "            This dataframe contains meta-data for search results, \n",
    "            must contain the columns 'code' and 'url'.\n",
    "        query2emb_func : callable\n",
    "            This is a function that takes as input a string and returns a vector\n",
    "            that is in the same vector space as what is loaded into the search index.\n",
    "\n",
    "        \"\"\"\n",
    "        assert 'url' in ref_df.columns\n",
    "        assert 'content' in ref_df.columns\n",
    "        \n",
    "        self.search_index = nmslib_index\n",
    "        self.ref_df = ref_df\n",
    "        self.query2emb_func = query2emb_func\n",
    "    \n",
    "    def search(self, str_search, k=1000):\n",
    "        \"\"\"\n",
    "        Prints the code that are the nearest neighbors (by cosine distance)\n",
    "        to the search query.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        str_search : str\n",
    "            a search query.  Ex: \"read data into pandas dataframe\"\n",
    "        k : int\n",
    "            the number of nearest neighbors to return.  Defaults to 2.\n",
    "        \n",
    "        \"\"\"\n",
    "        query = self.query2emb_func(str_search)\n",
    "        return self.search_index.knnQuery(query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = search_engine(nmslib_index=search_index,\n",
    "                   ref_df=ref_df,\n",
    "                   query2emb_func=q2emb.emb_mean)\n",
    "\n",
    "def rank(input):\n",
    "    idx, dist = input\n",
    "    vote = ref_df.iloc[idx].vote\n",
    "    polarity = ref_df.iloc[idx].sentiment_polarity\n",
    "    subjectivity = ref_df.iloc[idx].sentiment_subjectivity\n",
    "    return dist +  0.01/vote + 0.001/polarity\n",
    "\n",
    "def search(se, query):    \n",
    "    idxs, dists = se.search(query)\n",
    "    ranked_results = sorted(zip(idxs, dists), key=rank)\n",
    "    for idx, dist in ranked_results[:10]:    \n",
    "        content = ref_df.iloc[idx].content\n",
    "        url = ref_df.iloc[idx].url\n",
    "        vote = ref_df.iloc[idx].vote\n",
    "        polarity = ref_df.iloc[idx].sentiment_polarity\n",
    "        subjectivity = ref_df.iloc[idx].sentiment_subjectivity\n",
    "        score = dist + 0.01/vote + 0.001/polarity\n",
    "        print(content)\n",
    "        print(f'score: {score}')\n",
    "        print(f'url: {url}\\n---------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1 rows\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another solution is to use a proxy for the d_file .\n",
      "score: 0.23250860159443879\n",
      "url: https://stackoverflow.com/a/11521614\n",
      "---------------\n",
      "\n",
      "a pretty common way to communicate between a webpage and a python program is to run the python as a wsgi server . effectively the python program is a separate server which communicates with the webpage using gets and posts .\n",
      "score: 0.2354370473625922\n",
      "url: https://stackoverflow.com/a/43161022\n",
      "---------------\n",
      "\n",
      "one way to solve the issue is to start the container using a host network mode\n",
      "score: 0.23627890943989055\n",
      "url: https://stackoverflow.com/a/46407386\n",
      "---------------\n",
      "\n",
      "you could set up a simple ssh tunnel on a remote machine :\n",
      "score: 0.23661329907125153\n",
      "url: https://stackoverflow.com/a/10880851\n",
      "---------------\n",
      "\n",
      "you can start your server with following command :\n",
      "score: 0.24460984548008444\n",
      "url: https://stackoverflow.com/a/6111790\n",
      "---------------\n",
      "\n",
      "you can run a web browser or web control within xvfb , and use something like to capture it .\n",
      "score: 0.2627800579720522\n",
      "url: https://stackoverflow.com/a/2631881\n",
      "---------------\n",
      "\n",
      "you can execute a search instead of a single issue get.https://my_jira.atlassian.net/rest/api/2/search?jql=project=pro-key\n",
      "score: 0.2672370103600287\n",
      "url: https://stackoverflow.com/a/46538664\n",
      "---------------\n",
      "\n",
      "the simpliest way for a noobie to run a webserver is probably to use flask : http://flask.pocoo.org/\n",
      "score: 0.26725246529140073\n",
      "url: https://stackoverflow.com/a/31730847\n",
      "---------------\n",
      "\n",
      "honestly , i 'd start with classic lamp . take a stock apache server , and a mysql database , and put your python scripts in the cgi - bin directory . the fact that they 're sending and receiving json instead of http does n't make much difference .\n",
      "score: 0.269196352909729\n",
      "url: https://stackoverflow.com/a/12963229\n",
      "---------------\n",
      "\n",
      "here 's a python script to access a gmail account . first you need to generate an oauth token . download google 's xoauth.py module and run it . it will walk you through the steps . you 'll get a url to obtain a verification code -- paste this into the script and it will spit out your token and secret :\n",
      "score: 0.27241733820905084\n",
      "url: https://stackoverflow.com/a/5358289\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search(se, \"start a web server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1 rows\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the documentation : manage data in containers\n",
      "score: 0.12819459751618018\n",
      "url: https://stackoverflow.com/a/35772167\n",
      "---------------\n",
      "\n",
      "you can read data in chunks :\n",
      "score: 0.1290179330433333\n",
      "url: https://stackoverflow.com/a/44203901\n",
      "---------------\n",
      "\n",
      "you can read the json file all in once like :\n",
      "score: 0.15426600991689515\n",
      "url: https://stackoverflow.com/a/44381211\n",
      "---------------\n",
      "\n",
      "you can read the file as two separate parts ( stats and csv )\n",
      "score: 0.15745706546333071\n",
      "url: https://stackoverflow.com/a/44666621\n",
      "---------------\n",
      "\n",
      "first read csv into pandas df :\n",
      "score: 0.15783050736039206\n",
      "url: https://stackoverflow.com/a/44158879\n",
      "---------------\n",
      "\n",
      "read the csvs 's in using pd.read_csv(file )\n",
      "score: 0.16292514826609952\n",
      "url: https://stackoverflow.com/a/28818016\n",
      "---------------\n",
      "\n",
      "you can easily read your data into a dictionary of dictionaries :\n",
      "score: 0.16328864815394495\n",
      "url: https://stackoverflow.com/a/44234874\n",
      "---------------\n",
      "\n",
      "you can read from multiple sheets with pandas :\n",
      "score: 0.16338374934089014\n",
      "url: https://stackoverflow.com/a/34413753\n",
      "---------------\n",
      "\n",
      "read the binary file content like this :\n",
      "score: 0.16442439329724354\n",
      "url: https://stackoverflow.com/a/8711061\n",
      "---------------\n",
      "\n",
      "read the input & error stream in separated threads .\n",
      "score: 0.16727352971229933\n",
      "url: https://stackoverflow.com/a/9021521\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search(se, \"read data into dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1 rows\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index the string :\n",
      "score: 0.18749885437462094\n",
      "url: https://stackoverflow.com/a/46774736\n",
      "---------------\n",
      "\n",
      "draw keypoints as filled white circles :\n",
      "score: 0.18912973460082574\n",
      "url: https://stackoverflow.com/a/47157299\n",
      "---------------\n",
      "\n",
      "creating random sample data :\n",
      "score: 0.22811340574390707\n",
      "url: https://stackoverflow.com/a/34001321\n",
      "---------------\n",
      "\n",
      "find affine transformation to make rectangle axis - aligned . it is just rotation by angle\n",
      "score: 0.22844695922053726\n",
      "url: https://stackoverflow.com/a/48354958\n",
      "---------------\n",
      "\n",
      "i have tried applying a gaussian blur then processing it with adaptive thresholding and result removed noise in the image and blurriness .\n",
      "score: 0.2285950980838155\n",
      "url: https://stackoverflow.com/a/51081432\n",
      "---------------\n",
      "\n",
      "draw a line segment between those points :\n",
      "score: 0.2306625744563891\n",
      "url: https://stackoverflow.com/a/10573237\n",
      "---------------\n",
      "\n",
      "save the bins and use pd.cut again :\n",
      "score: 0.23214419030079306\n",
      "url: https://stackoverflow.com/a/42749333\n",
      "---------------\n",
      "\n",
      "question : loop through multidimensional dictionary and calculate\n",
      "score: 0.2321720354162319\n",
      "url: https://stackoverflow.com/a/44568827\n",
      "---------------\n",
      "\n",
      "get dataframe integer index given a date key :\n",
      "score: 0.23746025616414987\n",
      "url: https://stackoverflow.com/a/41070130\n",
      "---------------\n",
      "\n",
      "words below the frequency are dropped before training occurs . so , the relevant context window is the word - distance among surviving words .\n",
      "score: 0.23830468695074195\n",
      "url: https://stackoverflow.com/a/50730108\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search(se, 'plot time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
