{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download pre-processed data if you want to run from tutorial from this step.##\n",
    "from general_utils import get_step2_prerequisite_files\n",
    "\n",
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data/stackoverflow/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Language Model From Stackoverflow post content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,cv2\n",
    "from lang_model_utils import lm_vocab, load_lm_vocab, train_lang_model\n",
    "from general_utils import save_file_pickle, load_file_pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_path = Path('./data/stackoverflow/processed_data/')\n",
    "\n",
    "with open(source_path/'train.content_token', 'r') as f:\n",
    "    trn_raw = f.readlines()\n",
    "\n",
    "with open(source_path/'valid.content_token', 'r') as f:\n",
    "    val_raw = f.readlines()\n",
    "    \n",
    "with open(source_path/'test.content_token', 'r') as f:\n",
    "    test_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data for language model\n",
    "\n",
    "We will use the class  `build_lm_vocab` to prepare our data for the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 5,842 rows\n",
      "WARNING:root:Vocab Size 1,520\n",
      "WARNING:root:Transforming 5,842 rows.\n",
      "WARNING:root:Removed 114 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "vocab = lm_vocab(max_vocab=50000,\n",
    "                 min_freq=10)\n",
    "\n",
    "# fit the transform on the training data, then transform\n",
    "trn_flat_idx = vocab.fit_transform_flattened(trn_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   7, 1470,  211,  354,   21,   16,   34,    1,   12,  672])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_flat_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_xbos_',\n",
       " 'unfortunately',\n",
       " 'right',\n",
       " 'now',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " '_pad_',\n",
       " 'to',\n",
       " 'filter']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.itos[x] for x in trn_flat_idx[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Transforming 1,283 rows.\n",
      "WARNING:root:Removed 8 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "# apply transform to validation data\n",
    "val_flat_idx = vocab.transform_flattened(val_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Saved vocab to data/stackoverflow/lang_model/vocab.cls\n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    vocab.save('./data/stackoverflow/lang_model/vocab.cls')\n",
    "    save_file_pickle('./data/stackoverflow/lang_model/trn_flat_idx_list.pkl', trn_flat_idx)\n",
    "    save_file_pickle('./data/stackoverflow/lang_model/val_flat_idx_list.pkl', val_flat_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Fast.AI Language Model\n",
    "\n",
    "This model will read in files that were created and train a [fast.ai](https://github.com/fastai/fastai/tree/master/fastai) language model.  This model learns to predict the next word in the sentence using fast.ai's implementation of [AWD LSTM](https://github.com/salesforce/awd-lstm-lm).  \n",
    "\n",
    "The goal of training this model is to build a general purpose feature extractor for text that can be used in downstream models.  In this case, we will utilize this model to produce embeddings for function docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loaded vocab of size 1,520\n"
     ]
    }
   ],
   "source": [
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')\n",
    "trn_flat_idx = load_file_pickle('./data/stackoverflow/lang_model/trn_flat_idx_list.pkl')\n",
    "val_flat_idx = load_file_pickle('./data/stackoverflow/lang_model/val_flat_idx_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d590c9214e934e1ab7d286695a73ee9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      5.289824   5.073163  \n",
      "    1      4.991432   4.777453                            \n",
      "    2      4.633141   4.359899                            \n",
      "    3      4.261553   4.03212                             \n",
      "    4      3.991656   3.857163                            \n",
      "    5      3.810928   3.743084                            \n",
      "                                                          \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:State dict for the best model saved here:\n",
      "data/stackoverflow/lang_model_weights/models/langmodel_best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6      3.688868   3.666875  \n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner, lang_model = train_lang_model(model_path = './data/stackoverflow/lang_model_weights',\n",
    "                                                  trn_indexed = trn_flat_idx,\n",
    "                                                  val_indexed = val_flat_idx,\n",
    "                                                  vocab_size = vocab.vocab_size,\n",
    "                                                  lr=3e-3,\n",
    "                                                  em_sz= 500,\n",
    "                                                  nh= 500,\n",
    "                                                  bptt=20,\n",
    "                                                  cycle_len=1,\n",
    "                                                  n_cycle=3,\n",
    "                                                  cycle_mult=2,\n",
    "                                                  bs = 200,\n",
    "                                                  wd = 1e-6)\n",
    "    \n",
    "elif use_cache:    \n",
    "    logging.warning('Not re-training language model because use_cache=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0004c4a8c849d187ca4ce88dea5008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.53591    3.620921  \n",
      "    1      3.457347   3.601915                            \n",
      "    2      3.446788   3.576479                            \n",
      "    3      3.397346   3.554816                            \n",
      "    4      3.389691   3.543317                            \n",
      "    5      3.332148   3.523645                            \n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner.fit(1e-3, 3, wds=1e-6, cycle_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf40353656cd431780de3b9c97d1ab3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.336175   3.512969  \n",
      "    1      3.282112   3.495233                            \n",
      "    2      3.234097   3.486641                            \n",
      "    3      3.245711   3.495026                            \n",
      "    4      3.207955   3.48001                             \n",
      "    5      3.157338   3.463748                            \n",
      "    6      3.100554   3.447244                            \n",
      "    7      3.061632   3.443354                            \n",
      "    8      3.032302   3.435668                            \n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner.fit(1e-3, 2, wds=1e-6, cycle_len=3, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483e03e214164aa08501167aa72e3bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      3.085778   3.455039  \n",
      "    1      3.026549   3.433519                            \n",
      "    2      3.001236   3.44187                             \n",
      "    3      3.034877   3.44887                             \n",
      "    4      3.008628   3.45341                             \n",
      "    5      2.96828    3.451785                            \n",
      "    6      2.918167   3.444601                            \n",
      "    7      2.881839   3.450093                            \n",
      "    8      2.815383   3.453834                            \n",
      "    9      2.763045   3.47167                             \n",
      "    10     2.714031   3.4795                              \n",
      "    11     2.677741   3.485371                            \n",
      "    12     2.620355   3.487961                            \n",
      "    13     2.597635   3.487397                            \n",
      "    14     2.557454   3.507987                            \n",
      "    15     2.544845   3.505275                            \n",
      "    16     2.546671   3.511955                            \n",
      "    17     2.476752   3.522221                            \n",
      "    18     2.423061   3.527315                            \n",
      "    19     2.396002   3.535564                            \n",
      "    20     2.350339   3.552823                            \n",
      "    21     2.352826   3.553019                            \n",
      "    22     2.370458   3.553591                            \n",
      "    23     2.304892   3.561486                            \n",
      "    24     2.308496   3.567496                            \n",
      "    25     2.308106   3.5556                              \n",
      "    26     2.262708   3.567409                            \n",
      "    27     2.221131   3.573485                            \n",
      "    28     2.248931   3.579404                            \n",
      "    29     2.280711   3.573639                            \n",
      "    30     2.303136   3.574092                            \n",
      "    31     2.269417   3.577163                            \n",
      "    32     2.235373   3.573414                            \n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner.fit(1e-3, 2, wds=1e-6, cycle_len=3, cycle_mult=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save language model and learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner.save('lang_model_learner.fai')\n",
    "    lang_model_new = fastai_learner.model.eval()\n",
    "#     torch.save(lang_model_new, './data/stackoverflow/lang_model/lang_model_gpu_v2.torch')\n",
    "    torch.save(lang_model_new.cpu(), './data/stackoverflow/lang_model/lang_model_cpu.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Encode All Docstrings\n",
    "\n",
    "Now that we have trained the language model, the next step is to use the language model to encode all of the docstrings into a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note that checkpointed versions of the language model artifacts are available for download: **\n",
    "\n",
    "1. `lang_model_cpu_v2.torch` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model/lang_model_cpu_v2.torch \n",
    "2. `lang_model_gpu_v2.torch` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model/lang_model_gpu_v2.torch\n",
    "3. `vocab_v2.cls` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model/vocab_v2.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loaded vocab of size 1,520\n",
      "WARNING:root:Processing 7,125 rows\n"
     ]
    }
   ],
   "source": [
    "from lang_model_utils import load_lm_vocab\n",
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')\n",
    "idx_docs = vocab.transform(trn_raw + val_raw, max_seq_len=30, padding=False)\n",
    "lang_model = torch.load('./data/stackoverflow/lang_model/lang_model_cpu.torch', \n",
    "                        map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(1520, 500, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(1520, 500, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=500, out_features=1520, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the below code extracts embeddings for docstrings one docstring at a time, which is very inefficient.  Ideally, you want to extract embeddings in batch but account for the fact that you will have padding, etc. when extracting the hidden states.  For this tutorial, we only provide this minimal example, however you are welcome to improve upon this and sumbit a PR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2arr(l):\n",
    "    \"Convert list into pytorch Variable.\"\n",
    "    return V(np.expand_dims(np.array(l), -1)).cpu()\n",
    "\n",
    "def make_prediction_from_list(model, l):\n",
    "    \"\"\"\n",
    "    Encode a list of integers that represent a sequence of tokens.  The\n",
    "    purpose is to encode a sentence or phrase.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    model : fastai language model\n",
    "    l : list\n",
    "        list of integers, representing a sequence of tokens that you want to encode\n",
    "\n",
    "    \"\"\"\n",
    "    arr = list2arr(l)# turn list into pytorch Variable with bs=1\n",
    "    model.reset()  # language model is stateful, so you must reset upon each prediction\n",
    "    hidden_states = model(arr)[-1][-1] # RNN Hidden Layer output is last output, and only need the last layer\n",
    "\n",
    "    #return avg-pooling, max-pooling, and last hidden state\n",
    "    return hidden_states.mean(0), hidden_states.max(0)[0], hidden_states[-1]\n",
    "\n",
    "\n",
    "def get_embeddings(lm_model, list_list_int):\n",
    "    \"\"\"\n",
    "    Vectorize a list of sequences List[List[int]] using a fast.ai language model.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    lm_model : fastai language model\n",
    "    list_list_int : List[List[int]]\n",
    "        A list of sequences to encode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (avg, mean, last)\n",
    "        A tuple that returns the average-pooling, max-pooling over time steps as well as the last time step.\n",
    "    \"\"\"\n",
    "    n_rows = len(list_list_int)\n",
    "    n_dim = lm_model[0].nhid\n",
    "    avgarr = np.empty((n_rows, n_dim))\n",
    "    maxarr = np.empty((n_rows, n_dim))\n",
    "    lastarr = np.empty((n_rows, n_dim))\n",
    "\n",
    "    for i in tqdm_notebook(range(len(list_list_int))):\n",
    "        avg_, max_, last_ = make_prediction_from_list(lm_model, list_list_int[i])\n",
    "        avgarr[i,:] = avg_.data.numpy()\n",
    "        maxarr[i,:] = max_.data.numpy()\n",
    "        lastarr[i,:] = last_.data.numpy()\n",
    "\n",
    "    return avgarr, maxarr, lastarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a61e270ac74e1da7c2e46156ce42d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 39s, sys: 4.27 s, total: 20min 43s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg_hs, max_hs, last_hs = get_embeddings(lang_model, idx_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same thing for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1,065 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d56c601e8ac43c7bc05a4f3cb39668b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1065), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_docs_test = vocab.transform(test_raw, max_seq_len=30, padding=False)\n",
    "avg_hs_test, max_hs_test, last_hs_test = get_embeddings(lang_model, idx_docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Language Model Embeddings For Docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path('./data/stackoverflow/lang_model_emb/')\n",
    "np.save(savepath/'avg_emb_dim500.npy', avg_hs)\n",
    "np.save(savepath/'max_emb_dim500.npy', max_hs)\n",
    "np.save(savepath/'last_emb_dim500.npy', last_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test set embeddings also\n",
    "np.save(savepath/'avg_emb_dim500_test.npy', avg_hs_test)\n",
    "np.save(savepath/'max_emb_dim500_test.npy', max_hs_test)\n",
    "np.save(savepath/'last_emb_dim500_test.npy', last_hs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note that the embeddings saved to disk above have also been cached and are are available for download: **\n",
    "\n",
    "Train + Validation docstrings vectorized:\n",
    "\n",
    "1. `avg_emb_dim500_v2.npy` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/avg_emb_dim500_v2.npy\n",
    "2. `max_emb_dim500_v2.npy` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/last_emb_dim500_v2.npy\n",
    "3. `last_emb_dim500_v2.npy` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/max_emb_dim500_v2.npy\n",
    "\n",
    "Test set docstrings vectorized:\n",
    "\n",
    "1. `avg_emb_dim500_test_v2.npy`: https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/avg_emb_dim500_test_v2.npy\n",
    "\n",
    "2. `max_emb_dim500_test_v2.npy`: https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/last_emb_dim500_test_v2.npy\n",
    "\n",
    "3. `last_emb_dim500_test_v2.npy`: https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/max_emb_dim500_test_v2.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Sentence Embeddings\n",
    "\n",
    "One popular way of evaluating sentence embeddings is to measure the efficacy of these embeddings in downstream tasks like sentiment analysis, textual similarity etc.  Usually you can use general-purpose benchmarks such as the examples outlined [here](https://github.com/facebookresearch/SentEval) to measure the quality of your embeddings.  However, since this is a very domain specific dataset - those general purpose benchmarks may not be appropriate.  Unfortunately, we have not designed downstream tasks that we can open source at this point.\n",
    "\n",
    "In the absence of these downstream tasks, we can at least sanity check that these embeddings contain semantic information by doing the following:\n",
    "\n",
    "1. Manually examine similarity between sentences, by supplying a statement and examining if the nearest phrase found is similar. \n",
    "\n",
    "2. Visualize the embeddings.\n",
    "\n",
    "We will do the first approach, and leave the second approach as an exercise for the reader.  **It should be noted that this is only a sanity check -- a more rigorous approach is to measure the impact of these embeddings on a variety of downstream tasks** and use that to form a more objective opinion about the quality of your embeddings.\n",
    "\n",
    "Furthermroe, there are many different ways of constructing a sentence embedding from the language model.  For example, we can take the average, the maximum or even the last value of the hidden states (or concatenate them all together).  **For simplicity, we will only evaluate the sentence embedding that is constructed by taking the average over the hidden states** (and leave other possibilities as an exercise for the reader). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search index using `nmslib` \n",
    "\n",
    "[nmslib](https://github.com/nmslib/nmslib) is a great library for doing nearest neighbor lookups, which we will use as a search engine for finding nearest neighbors of comments in vector-space.  \n",
    "\n",
    "The convenience function `create_nmslib_search_index` builds this search index given a matrix of vectors as input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import create_nmslib_search_index\n",
    "import nmslib\n",
    "from lang_model_utils import Query2Emb\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lang_model_utils import load_lm_vocab\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matrix of vectors\n",
    "loadpath = Path('./data/stackoverflow/lang_model_emb/')\n",
    "avg_emb_dim500 = np.load(loadpath/'avg_emb_dim500_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build search index (takes about an hour on a p3.8xlarge)\n",
    "dim500_avg_searchindex = create_nmslib_search_index(avg_emb_dim500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save search index\n",
    "dim500_avg_searchindex.saveIndex('./data/stackoverflow/lang_model_emb/dim500_avg_searchindex.nmslib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you did not train your own language model and are downloading the pre-trained model artifacts instead, you can similarly download the pre-computed search index here: \n",
    "\n",
    "https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/dim500_avg_searchindex.nmslib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have built this search index with nmslib, you can do fast nearest-neighbor lookups.  We use the `Query2Emb` object to help convert strings to the embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim500_avg_searchindex = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "dim500_avg_searchindex.loadIndex('./data/stackoverflow/lang_model_emb/dim500_avg_searchindex.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loaded vocab of size 1,520\n",
      "WARNING:root:Processing 1 rows\n"
     ]
    }
   ],
   "source": [
    "lang_model = torch.load('./data/stackoverflow/lang_model/lang_model_cpu.torch')\n",
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')\n",
    "\n",
    "q2emb = Query2Emb(lang_model = lang_model.cpu(),\n",
    "                  vocab = vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `Query2Emb.emb_mean` will allow us to use the langauge model we trained earlier to generate a sentence embedding given a string.   Here is an example, `emb_mean` will return a numpy array of size (1, 500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 1 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = q2emb.emb_mean('Read data into pandas dataframe')\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make search engine to inspect semantic similarity of phrases**.  This will take 3 inputs:\n",
    "\n",
    "1. `nmslib_index` - this is the search index we built above.  This object takes a vector and will return the index of the closest vector(s) according to cosine distance.  \n",
    "2. `ref_data` - this is the data for which the index refer to, in this case will be the docstrings. \n",
    "3. `query2emb_func` - this is a function that will convert a string into an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_engine:\n",
    "    def __init__(self, \n",
    "                 nmslib_index, \n",
    "                 ref_data, \n",
    "                 query2emb_func):\n",
    "        \n",
    "        self.search_index = nmslib_index\n",
    "        self.data = ref_data\n",
    "        self.query2emb_func = query2emb_func\n",
    "    \n",
    "    def search(self, str_search, k=10):\n",
    "        query = self.query2emb_func(str_search)\n",
    "        idxs, dists = self.search_index.knnQuery(query, k=k)\n",
    "        \n",
    "        for idx, dist in zip(idxs, dists):\n",
    "            print(f'cosine dist:{dist:.4f}\\n---------------\\n', self.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = search_engine(nmslib_index=dim500_avg_searchindex,\n",
    "                   ref_data = test_raw,\n",
    "                   query2emb_func = q2emb.emb_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Inspect Phrase Similarity\n",
    "\n",
    "Compare a user-supplied query vs. vectorized docstrings on test set.  We can see that similar phrases are not exactly the same, but the nearest neighbors are reasonable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.2382\n",
      "---------------\n",
      " read your file into a numpy array and use numpy broadcast feature :\n",
      "\n",
      "cosine dist:0.2522\n",
      "---------------\n",
      " starting with this :\n",
      "\n",
      "cosine dist:0.2800\n",
      "---------------\n",
      " install this package using :\n",
      "\n",
      "cosine dist:0.2863\n",
      "---------------\n",
      " if you want to convert a python dictionary to json using the json.dumps ( ) method .\n",
      "\n",
      "cosine dist:0.2909\n",
      "---------------\n",
      " reading up on floating point numbers may be worth while : https://docs.python.org/3/tutorial/floatingpoint.html\n",
      "\n",
      "cosine dist:0.3021\n",
      "---------------\n",
      " use list append for adding items into list\n",
      "\n",
      "cosine dist:0.3036\n",
      "---------------\n",
      " install tesseract from https://github.com/ub-mannheim/tesseract/wiki and add the path of tesseract.exe to the path environment variable .\n",
      "\n",
      "cosine dist:0.3074\n",
      "---------------\n",
      " use zip :\n",
      "\n",
      "cosine dist:0.3214\n",
      "---------------\n",
      " \"use httpresponseredirect instead of redirect function ,\"\n",
      "\n",
      "cosine dist:0.3256\n",
      "---------------\n",
      " \"as @jonrsharpe mentions , use actual datetime objects . here i define a / is_next_month method . both use datetime.strptime to convert your strings to datetime objects.from datetime import * def is_next_week(date_str ) : new_date = datetime.strptime(date_str , \"\" % m/%d/%y\"\").date ( ) today_date = datetime.today ( ) return ( new_date.isocalendar()[1 ] - today_date.isocalendar()[1 ] ) + ( new_date.year - today_date.year ) * today_date.isocalendar()[1 ] = = 1 def is_next_month(date_str ) : new_date = datetime.strptime(date_str , \"\" % m/%d/%y\"\").date ( ) today_date = datetime.today ( ) return ( new_date.month - today_date.month ) + ( new_date.year - today_date.year ) * today_date.month = = 1 num_d = { ' random next month day ' : ' 08/28/17 ' , ' just another day ' : ' 08/23/17 ' , ' next week day ' : ' 07/18/17 ' , ' not in next week ' : ' 07/25/17 ' } print \"\" date week month \"\" for key in num_d : print num_d[key ] , is_next_week(num_d[key ] ) , is_next_month(num_d[key ] )\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('read csv into pandas dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.3063\n",
      "---------------\n",
      " 1.install scrapy for your python version\n",
      "\n",
      "cosine dist:0.3514\n",
      "---------------\n",
      " starting with this :\n",
      "\n",
      "cosine dist:0.3614\n",
      "---------------\n",
      " javax.net.ssl.sslhandshakeexception : no cipher suits in common\n",
      "\n",
      "cosine dist:0.3845\n",
      "---------------\n",
      " ipopt has a bunch of different convergence tolerances . check out some documentation at : http://www.coin-or.org/ipopt/documentation/node42.html\n",
      "\n",
      "cosine dist:0.3906\n",
      "---------------\n",
      " \"including in a path means \"\" from the current directory \"\" .\"\n",
      "\n",
      "cosine dist:0.3945\n",
      "---------------\n",
      " stick with one service(either resource or client ) .\n",
      "\n",
      "cosine dist:0.4113\n",
      "---------------\n",
      " accordind to your own code :\n",
      "\n",
      "cosine dist:0.4120\n",
      "---------------\n",
      " output :\n",
      "\n",
      "cosine dist:0.4157\n",
      "---------------\n",
      " straight forward :\n",
      "\n",
      "cosine dist:0.4233\n",
      "---------------\n",
      " your original function returns a numeric value .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('train a random forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1265\n",
      "---------------\n",
      " use glob.glob\n",
      "\n",
      "cosine dist:0.1859\n",
      "---------------\n",
      " prints\n",
      "\n",
      "cosine dist:0.1880\n",
      "---------------\n",
      " use zip :\n",
      "\n",
      "cosine dist:0.1964\n",
      "---------------\n",
      " output :\n",
      "\n",
      "cosine dist:0.1985\n",
      "---------------\n",
      " edit :\n",
      "\n",
      "cosine dist:0.2099\n",
      "---------------\n",
      " use this urlpattern\n",
      "\n",
      "cosine dist:0.2269\n",
      "---------------\n",
      " run php script in background\n",
      "\n",
      "cosine dist:0.2475\n",
      "---------------\n",
      " straight forward :\n",
      "\n",
      "cosine dist:0.2574\n",
      "---------------\n",
      " install this package using :\n",
      "\n",
      "cosine dist:0.2610\n",
      "---------------\n",
      " data.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('download files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1899\n",
      "---------------\n",
      " straight forward :\n",
      "\n",
      "cosine dist:0.2053\n",
      "---------------\n",
      " idp generic means\n",
      "\n",
      "cosine dist:0.2181\n",
      "---------------\n",
      " data.json\n",
      "\n",
      "cosine dist:0.2181\n",
      "---------------\n",
      " michael\n",
      "\n",
      "cosine dist:0.2307\n",
      "---------------\n",
      " output :\n",
      "\n",
      "cosine dist:0.2516\n",
      "---------------\n",
      " things changes :\n",
      "\n",
      "cosine dist:0.2702\n",
      "---------------\n",
      " prints\n",
      "\n",
      "cosine dist:0.2732\n",
      "---------------\n",
      " for me it works nice :\n",
      "\n",
      "cosine dist:0.2753\n",
      "---------------\n",
      " code\n",
      "\n",
      "cosine dist:0.2831\n",
      "---------------\n",
      " iiuc you want :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('start webserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.2420\n",
      "---------------\n",
      " \"use httpresponseredirect instead of redirect function ,\"\n",
      "\n",
      "cosine dist:0.2597\n",
      "---------------\n",
      " try model structure syntax :\n",
      "\n",
      "cosine dist:0.2649\n",
      "---------------\n",
      " reading up on floating point numbers may be worth while : https://docs.python.org/3/tutorial/floatingpoint.html\n",
      "\n",
      "cosine dist:0.2823\n",
      "---------------\n",
      " you have to convert your latitudes ans longitudes to map projection before calling scatter :\n",
      "\n",
      "cosine dist:0.2872\n",
      "---------------\n",
      " \"each file is a module . for a module to access another module 's content , it needs to import it first.from mysecondclass import mysecondclass x = mysecondclass ( )\"\n",
      "\n",
      "cosine dist:0.2973\n",
      "---------------\n",
      " you need to change this part :\n",
      "\n",
      "cosine dist:0.2973\n",
      "---------------\n",
      " telegram api names it as . you can send message to chat_id which goes to private chat .\n",
      "\n",
      "cosine dist:0.3023\n",
      "---------------\n",
      " you need to pass some of the information along with the request to the server . following code should work ... you can play along with other parameter as well\n",
      "\n",
      "cosine dist:0.3044\n",
      "---------------\n",
      " get rid of the in testingnow.runa ( ) and testingnow.runb ( ) .\n",
      "\n",
      "cosine dist:0.3055\n",
      "---------------\n",
      " first you need to pass the tables names return from nav_bar function to your render template like this :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('send out email notification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1796\n",
      "---------------\n",
      " output :\n",
      "\n",
      "cosine dist:0.1954\n",
      "---------------\n",
      " use glob.glob\n",
      "\n",
      "cosine dist:0.1987\n",
      "---------------\n",
      " prints\n",
      "\n",
      "cosine dist:0.2077\n",
      "---------------\n",
      " use zip :\n",
      "\n",
      "cosine dist:0.2395\n",
      "---------------\n",
      " use this urlpattern\n",
      "\n",
      "cosine dist:0.2468\n",
      "---------------\n",
      " edit :\n",
      "\n",
      "cosine dist:0.2548\n",
      "---------------\n",
      " starting with this :\n",
      "\n",
      "cosine dist:0.2803\n",
      "---------------\n",
      " run php script in background\n",
      "\n",
      "cosine dist:0.2877\n",
      "---------------\n",
      " straight forward :\n",
      "\n",
      "cosine dist:0.2966\n",
      "---------------\n",
      " change the following code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('save file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Embeddings (Optional)\n",
    "\n",
    "We highly recommend using [tensorboard](https://www.tensorflow.org/versions/r1.0/get_started/embedding_viz) as way to visualize embeddings.  Tensorboard contains an interactive search that makes it easy (and fun) to explore embeddings.  We leave this as an exercise to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
