{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download pre-processed data if you want to run from tutorial from this step.##\n",
    "from general_utils import get_step2_prerequisite_files\n",
    "\n",
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data/stackoverflow/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Language Model From Stackoverflow post content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch,cv2\n",
    "from lang_model_utils import lm_vocab, load_lm_vocab, train_lang_model\n",
    "from general_utils import save_file_pickle, load_file_pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_path = Path('./data/stackoverflow/processed_data/')\n",
    "\n",
    "with open(source_path/'train.content_token', 'r') as f:\n",
    "    trn_raw = f.readlines()\n",
    "\n",
    "with open(source_path/'valid.content_token', 'r') as f:\n",
    "    val_raw = f.readlines()\n",
    "    \n",
    "with open(source_path/'test.content_token', 'r') as f:\n",
    "    test_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data for language model\n",
    "\n",
    "We will use the class  `build_lm_vocab` to prepare our data for the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 459,761 rows\n",
      "WARNING:root:Vocab Size 27,877\n",
      "WARNING:root:Transforming 459,761 rows.\n",
      "WARNING:root:Removed 11,685 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "vocab = lm_vocab(max_vocab=50000,\n",
    "                 min_freq=10)\n",
    "\n",
    "# fit the transform on the training data, then transform\n",
    "trn_flat_idx = vocab.fit_transform_flattened(trn_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7,  53, 368, 259, 146, 447,  23,  15, 308,   6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_flat_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_xbos_',\n",
       " 'an',\n",
       " 'approach',\n",
       " 'without',\n",
       " 'any',\n",
       " 'built',\n",
       " '-',\n",
       " 'in',\n",
       " 'functions',\n",
       " ':']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.itos[x] for x in trn_flat_idx[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Transforming 100,924 rows.\n",
      "WARNING:root:Removed 1,639 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "# apply transform to validation data\n",
    "val_flat_idx = vocab.transform_flattened(val_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Saved vocab to data/stackoverflow/lang_model/vocab.cls\n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    vocab.save('./data/stackoverflow/lang_model/vocab.cls')\n",
    "    save_file_pickle('./data/stackoverflow/lang_model/trn_flat_idx_list.pkl', trn_flat_idx)\n",
    "    save_file_pickle('./data/stackoverflow/lang_model/val_flat_idx_list.pkl', val_flat_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Fast.AI Language Model\n",
    "\n",
    "This model will read in files that were created and train a [fast.ai](https://github.com/fastai/fastai/tree/master/fastai) language model.  This model learns to predict the next word in the sentence using fast.ai's implementation of [AWD LSTM](https://github.com/salesforce/awd-lstm-lm).  \n",
    "\n",
    "The goal of training this model is to build a general purpose feature extractor for text that can be used in downstream models.  In this case, we will utilize this model to produce embeddings for function docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loaded vocab of size 27,877\n"
     ]
    }
   ],
   "source": [
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')\n",
    "trn_flat_idx = load_file_pickle('./data/stackoverflow/lang_model/trn_flat_idx_list.pkl')\n",
    "val_flat_idx = load_file_pickle('./data/stackoverflow/lang_model/val_flat_idx_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6df9f23834a42b6b79590384f71c334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                    \n",
      "    0      3.601241   3.474715  \n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:State dict for the best model saved here:\n",
      "data/stackoverflow/lang_model_weights/models/langmodel_best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      3.481606   3.358989  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner, lang_model = train_lang_model(model_path = './data/stackoverflow/lang_model_weights',\n",
    "                                                  trn_indexed = trn_flat_idx,\n",
    "                                                  val_indexed = val_flat_idx,\n",
    "                                                  vocab_size = vocab.vocab_size,\n",
    "                                                  lr=3e-3,\n",
    "                                                  em_sz= 500,\n",
    "                                                  nh= 500,\n",
    "                                                  bptt=20,\n",
    "                                                  cycle_len=1,\n",
    "                                                  n_cycle=2,\n",
    "                                                  bs = 200,\n",
    "                                                  wd = 1e-6)\n",
    "    \n",
    "elif use_cache:    \n",
    "    logging.warning('Not re-training language model because use_cache=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not use_cache:\n",
    "#     fastai_learner.fit(1e-3, 3, wds=1e-6, cycle_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e6a6f98164b1c9e75b0e2749957dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                    \n",
      "    0      3.472972   3.333834  \n",
      "    1      3.394681   3.286406                                    \n",
      "    2      3.385816   3.272807                                    \n",
      "    3      3.41238    3.283735                                    \n",
      " 68%|██████▊   | 3456/5086 [3:25:59<1:37:09,  3.58s/it, loss=3.36]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2384/5086 [2:25:33<2:44:58,  3.66s/it, loss=3.29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5      3.348129   3.232127                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner.fit(1e-3, 2, wds=1e-6, cycle_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not use_cache:\n",
    "#     fastai_learner.fit(1e-3, 2, wds=1e-6, cycle_len=3, cycle_mult=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save language model and learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_cache:\n",
    "    fastai_learner.save('lang_model_learner.fai')\n",
    "    lang_model_new = fastai_learner.model.eval()\n",
    "#     torch.save(lang_model_new, './data/stackoverflow/lang_model/lang_model_gpu_v2.torch')\n",
    "    torch.save(lang_model_new.cpu(), './data/stackoverflow/lang_model/lang_model_cpu.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Encode All Docstrings\n",
    "\n",
    "Now that we have trained the language model, the next step is to use the language model to encode all of the docstrings into a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note that checkpointed versions of the language model artifacts are available for download: **\n",
    "\n",
    "1. `lang_model_cpu_v2.torch` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model/lang_model_cpu_v2.torch \n",
    "2. `lang_model_gpu_v2.torch` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model/lang_model_gpu_v2.torch\n",
    "3. `vocab_v2.cls` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model/vocab_v2.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loaded vocab of size 27,877\n",
      "WARNING:root:Processing 560,685 rows\n"
     ]
    }
   ],
   "source": [
    "from lang_model_utils import load_lm_vocab\n",
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')\n",
    "idx_docs = vocab.transform(trn_raw + val_raw, max_seq_len=30, padding=False)\n",
    "lang_model = torch.load('./data/stackoverflow/lang_model/lang_model_cpu.torch', \n",
    "                        map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(27877, 500, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(27877, 500, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(500, 500)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=500, out_features=27877, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the below code extracts embeddings for docstrings one docstring at a time, which is very inefficient.  Ideally, you want to extract embeddings in batch but account for the fact that you will have padding, etc. when extracting the hidden states.  For this tutorial, we only provide this minimal example, however you are welcome to improve upon this and sumbit a PR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2arr(l):\n",
    "    \"Convert list into pytorch Variable.\"\n",
    "    return V(np.expand_dims(np.array(l), -1)).cpu()\n",
    "\n",
    "def make_prediction_from_list(model, l):\n",
    "    \"\"\"\n",
    "    Encode a list of integers that represent a sequence of tokens.  The\n",
    "    purpose is to encode a sentence or phrase.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    model : fastai language model\n",
    "    l : list\n",
    "        list of integers, representing a sequence of tokens that you want to encode\n",
    "\n",
    "    \"\"\"\n",
    "    arr = list2arr(l)# turn list into pytorch Variable with bs=1\n",
    "    model.reset()  # language model is stateful, so you must reset upon each prediction\n",
    "    hidden_states = model(arr)[-1][-1] # RNN Hidden Layer output is last output, and only need the last layer\n",
    "\n",
    "    #return avg-pooling, max-pooling, and last hidden state\n",
    "    return hidden_states.mean(0), hidden_states.max(0)[0], hidden_states[-1]\n",
    "\n",
    "\n",
    "def get_embeddings(lm_model, list_list_int):\n",
    "    \"\"\"\n",
    "    Vectorize a list of sequences List[List[int]] using a fast.ai language model.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    lm_model : fastai language model\n",
    "    list_list_int : List[List[int]]\n",
    "        A list of sequences to encode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (avg, mean, last)\n",
    "        A tuple that returns the average-pooling, max-pooling over time steps as well as the last time step.\n",
    "    \"\"\"\n",
    "    n_rows = len(list_list_int)\n",
    "    n_dim = lm_model[0].nhid\n",
    "    avgarr = np.empty((n_rows, n_dim))\n",
    "    maxarr = np.empty((n_rows, n_dim))\n",
    "    lastarr = np.empty((n_rows, n_dim))\n",
    "\n",
    "    for i in tqdm_notebook(range(len(list_list_int))):\n",
    "        avg_, max_, last_ = make_prediction_from_list(lm_model, list_list_int[i])\n",
    "        avgarr[i,:] = avg_.data.numpy()\n",
    "        maxarr[i,:] = max_.data.numpy()\n",
    "        lastarr[i,:] = last_.data.numpy()\n",
    "\n",
    "    return avgarr, maxarr, lastarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486fb178508f423e99e1ebbccd8f7ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=560685), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1d 7h 33min 42s, sys: 6min 46s, total: 1d 7h 40min 28s\n",
      "Wall time: 4h 31min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg_hs, max_hs, last_hs = get_embeddings(lang_model, idx_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same thing for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Processing 83,781 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132d3850b85c40dc8df42491654fe1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83781), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "idx_docs_test = vocab.transform(test_raw, max_seq_len=30, padding=False)\n",
    "avg_hs_test, max_hs_test, last_hs_test = get_embeddings(lang_model, idx_docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Language Model Embeddings For Docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path('./data/stackoverflow/lang_model_emb/')\n",
    "np.save(savepath/'avg_emb_dim500.npy', avg_hs)\n",
    "np.save(savepath/'max_emb_dim500.npy', max_hs)\n",
    "np.save(savepath/'last_emb_dim500.npy', last_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test set embeddings also\n",
    "np.save(savepath/'avg_emb_dim500_test.npy', avg_hs_test)\n",
    "np.save(savepath/'max_emb_dim500_test.npy', max_hs_test)\n",
    "np.save(savepath/'last_emb_dim500_test.npy', last_hs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note that the embeddings saved to disk above have also been cached and are are available for download: **\n",
    "\n",
    "Train + Validation docstrings vectorized:\n",
    "\n",
    "1. `avg_emb_dim500_v2.npy` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/avg_emb_dim500_v2.npy\n",
    "2. `max_emb_dim500_v2.npy` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/last_emb_dim500_v2.npy\n",
    "3. `last_emb_dim500_v2.npy` : https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/max_emb_dim500_v2.npy\n",
    "\n",
    "Test set docstrings vectorized:\n",
    "\n",
    "1. `avg_emb_dim500_test_v2.npy`: https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/avg_emb_dim500_test_v2.npy\n",
    "\n",
    "2. `max_emb_dim500_test_v2.npy`: https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/last_emb_dim500_test_v2.npy\n",
    "\n",
    "3. `last_emb_dim500_test_v2.npy`: https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/max_emb_dim500_test_v2.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Sentence Embeddings\n",
    "\n",
    "One popular way of evaluating sentence embeddings is to measure the efficacy of these embeddings in downstream tasks like sentiment analysis, textual similarity etc.  Usually you can use general-purpose benchmarks such as the examples outlined [here](https://github.com/facebookresearch/SentEval) to measure the quality of your embeddings.  However, since this is a very domain specific dataset - those general purpose benchmarks may not be appropriate.  Unfortunately, we have not designed downstream tasks that we can open source at this point.\n",
    "\n",
    "In the absence of these downstream tasks, we can at least sanity check that these embeddings contain semantic information by doing the following:\n",
    "\n",
    "1. Manually examine similarity between sentences, by supplying a statement and examining if the nearest phrase found is similar. \n",
    "\n",
    "2. Visualize the embeddings.\n",
    "\n",
    "We will do the first approach, and leave the second approach as an exercise for the reader.  **It should be noted that this is only a sanity check -- a more rigorous approach is to measure the impact of these embeddings on a variety of downstream tasks** and use that to form a more objective opinion about the quality of your embeddings.\n",
    "\n",
    "Furthermroe, there are many different ways of constructing a sentence embedding from the language model.  For example, we can take the average, the maximum or even the last value of the hidden states (or concatenate them all together).  **For simplicity, we will only evaluate the sentence embedding that is constructed by taking the average over the hidden states** (and leave other possibilities as an exercise for the reader). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search index using `nmslib` \n",
    "\n",
    "[nmslib](https://github.com/nmslib/nmslib) is a great library for doing nearest neighbor lookups, which we will use as a search engine for finding nearest neighbors of comments in vector-space.  \n",
    "\n",
    "The convenience function `create_nmslib_search_index` builds this search index given a matrix of vectors as input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import create_nmslib_search_index\n",
    "import nmslib\n",
    "from lang_model_utils import Query2Emb\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lang_model_utils import load_lm_vocab\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matrix of vectors\n",
    "loadpath = Path('./data/stackoverflow/lang_model_emb/')\n",
    "avg_emb_dim500 = np.load(loadpath/'avg_emb_dim500_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build search index (takes about an hour on a p3.8xlarge)\n",
    "dim500_avg_searchindex = create_nmslib_search_index(avg_emb_dim500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save search index\n",
    "dim500_avg_searchindex.saveIndex('./data/stackoverflow/lang_model_emb/dim500_avg_searchindex.nmslib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you did not train your own language model and are downloading the pre-trained model artifacts instead, you can similarly download the pre-computed search index here: \n",
    "\n",
    "https://storage.googleapis.com/kubeflow-examples/code_search/data/lang_model_emb/dim500_avg_searchindex.nmslib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have built this search index with nmslib, you can do fast nearest-neighbor lookups.  We use the `Query2Emb` object to help convert strings to the embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim500_avg_searchindex = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "dim500_avg_searchindex.loadIndex('./data/stackoverflow/lang_model_emb/dim500_avg_searchindex.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = torch.load('./data/stackoverflow/lang_model/lang_model_cpu.torch')\n",
    "vocab = load_lm_vocab('./data/stackoverflow/lang_model/vocab.cls')\n",
    "\n",
    "q2emb = Query2Emb(lang_model = lang_model.cpu(),\n",
    "                  vocab = vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `Query2Emb.emb_mean` will allow us to use the langauge model we trained earlier to generate a sentence embedding given a string.   Here is an example, `emb_mean` will return a numpy array of size (1, 500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = q2emb.emb_mean('Read data into pandas dataframe')\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make search engine to inspect semantic similarity of phrases**.  This will take 3 inputs:\n",
    "\n",
    "1. `nmslib_index` - this is the search index we built above.  This object takes a vector and will return the index of the closest vector(s) according to cosine distance.  \n",
    "2. `ref_data` - this is the data for which the index refer to, in this case will be the docstrings. \n",
    "3. `query2emb_func` - this is a function that will convert a string into an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_engine:\n",
    "    def __init__(self, \n",
    "                 nmslib_index, \n",
    "                 ref_data, \n",
    "                 query2emb_func):\n",
    "        \n",
    "        self.search_index = nmslib_index\n",
    "        self.data = ref_data\n",
    "        self.query2emb_func = query2emb_func\n",
    "    \n",
    "    def search(self, str_search, k=50):\n",
    "        query = self.query2emb_func(str_search)\n",
    "        idxs, dists = self.search_index.knnQuery(query, k=k)\n",
    "        \n",
    "        for idx, dist in zip(idxs, dists):\n",
    "            print(f'cosine dist:{dist:.4f}\\n---------------\\n', self.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = search_engine(nmslib_index=dim500_avg_searchindex,\n",
    "                   ref_data = test_raw,\n",
    "                   query2emb_func = q2emb.emb_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Inspect Phrase Similarity\n",
    "\n",
    "Compare a user-supplied query vs. vectorized docstrings on test set.  We can see that similar phrases are not exactly the same, but the nearest neighbors are reasonable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1122\n",
      "---------------\n",
      " first read csv into pandas df :\n",
      "\n",
      "cosine dist:0.1260\n",
      "---------------\n",
      " the easiest way is pandas . read data from file into dataframe :\n",
      "\n",
      "cosine dist:0.1292\n",
      "---------------\n",
      " read the documentation : manage data in containers\n",
      "\n",
      "cosine dist:0.1358\n",
      "---------------\n",
      " you can read from multiple sheets with pandas :\n",
      "\n",
      "cosine dist:0.1380\n",
      "---------------\n",
      " you can read data in chunks :\n",
      "\n",
      "cosine dist:0.1402\n",
      "---------------\n",
      " read the csvs 's in using pd.read_csv(file )\n",
      "\n",
      "cosine dist:0.1418\n",
      "---------------\n",
      " read your dataframe -\n",
      "\n",
      "cosine dist:0.1438\n",
      "---------------\n",
      " you can read the file as two separate parts ( stats and csv )\n",
      "\n",
      "cosine dist:0.1442\n",
      "---------------\n",
      " first read your csv file with pandas with\n",
      "\n",
      "cosine dist:0.1499\n",
      "---------------\n",
      " you can easily read your data into a dictionary of dictionaries :\n",
      "\n",
      "cosine dist:0.1501\n",
      "---------------\n",
      " you can read the json file all in once like :\n",
      "\n",
      "cosine dist:0.1565\n",
      "---------------\n",
      " read the input & error stream in separated threads .\n",
      "\n",
      "cosine dist:0.1587\n",
      "---------------\n",
      " load the data first :\n",
      "\n",
      "cosine dist:0.1592\n",
      "---------------\n",
      " read in binary mode :\n",
      "\n",
      "cosine dist:0.1627\n",
      "---------------\n",
      " you can read the csv file line by line download the content by updating the url . code :\n",
      "\n",
      "cosine dist:0.1629\n",
      "---------------\n",
      " for option 3 .. getting your data in efficiently ... read it all in as one long str type in python with a single read from the file . let 's assume it 's called mystr .\n",
      "\n",
      "cosine dist:0.1631\n",
      "---------------\n",
      " read the binary file content like this :\n",
      "\n",
      "cosine dist:0.1631\n",
      "---------------\n",
      " i would read the information from the file and put it in a list - of - lists like this :\n",
      "\n",
      "cosine dist:0.1643\n",
      "---------------\n",
      " read in your file :\n",
      "\n",
      "cosine dist:0.1657\n",
      "---------------\n",
      " you can load big sized csv files with pandas in a efficient way . someone made a benchmark on stackexchange . you can load data into data frames and then split data frames to small chunks with numpy . you can also save new data frames as csv file with pandas .\n",
      "\n",
      "cosine dist:0.1705\n",
      "---------------\n",
      " far from the smoothest of solutions but you could read the data in as dictionaries that you update and later insert to a dataframe .\n",
      "\n",
      "cosine dist:0.1708\n",
      "---------------\n",
      " according to this question you need to read the data from the in buffer in chunks ( here single byte ) :\n",
      "\n",
      "cosine dist:0.1714\n",
      "---------------\n",
      " use the csv module to read lines as csv rows .\n",
      "\n",
      "cosine dist:0.1730\n",
      "---------------\n",
      " another way is to read the rows from the csv and update the dictionary with each row :\n",
      "\n",
      "cosine dist:0.1737\n",
      "---------------\n",
      " read your excel file into a numpy array . this has already been answered before .\n",
      "\n",
      "cosine dist:0.1744\n",
      "---------------\n",
      " \"like what @aus_lacy has already suggested , you just need to read the csv file into a data frame first , concatenate two data frames and write it back to the csv file :\"\n",
      "\n",
      "cosine dist:0.1777\n",
      "---------------\n",
      " you can dump all results to the csv file without looping :\n",
      "\n",
      "cosine dist:0.1783\n",
      "---------------\n",
      " reading data from an temporary file - like object\n",
      "\n",
      "cosine dist:0.1791\n",
      "---------------\n",
      " you can read your file line by line\n",
      "\n",
      "cosine dist:0.1796\n",
      "---------------\n",
      " load the data into a string a then do\n",
      "\n",
      "cosine dist:0.1798\n",
      "---------------\n",
      " please read retrieving a subset of fields on the mongodb website .\n",
      "\n",
      "cosine dist:0.1802\n",
      "---------------\n",
      " first collect all the rows in the csv file in a list :\n",
      "\n",
      "cosine dist:0.1805\n",
      "---------------\n",
      " try reading the files into separate dictionary with lines numbers as keys . you can then iterate through both the dictionaries at the same time using zip function .\n",
      "\n",
      "cosine dist:0.1821\n",
      "---------------\n",
      " for example you can read all file line by line at once to get information which you need .\n",
      "\n",
      "cosine dist:0.1832\n",
      "---------------\n",
      " look at the docs for splitting an array into multiple sub - arrays .\n",
      "\n",
      "cosine dist:0.1839\n",
      "---------------\n",
      " here is an example from store some data from httpbin.org and storing data in a pickle file .\n",
      "\n",
      "cosine dist:0.1842\n",
      "---------------\n",
      " \"this example is tailor - made for using a pandas dataframe . you can read in your csv to a dataframe , sort by values of different columns , get subsets of your dataframe , etc . this is how i would do it for your example :\"\n",
      "\n",
      "cosine dist:0.1856\n",
      "---------------\n",
      " pandas can read gzipped data files with the ordinary . how to do a diff between two dataframes is described in pandas : diff of two dataframes .\n",
      "\n",
      "cosine dist:0.1857\n",
      "---------------\n",
      " \"i would recommend you read in the data frame by skipping rows , then create a dictionary to rename your columns .\"\n",
      "\n",
      "cosine dist:0.1873\n",
      "---------------\n",
      " trying reading csv file using csv library\n",
      "\n",
      "cosine dist:0.1876\n",
      "---------------\n",
      " you may read in the file into memory and use\n",
      "\n",
      "cosine dist:0.1878\n",
      "---------------\n",
      " \"read the numbers from the file and construct the matrix , with list comprehension\"\n",
      "\n",
      "cosine dist:0.1887\n",
      "---------------\n",
      " \"given that you have already written your data to two files , you can read those files from disk and zip every line together like this :\"\n",
      "\n",
      "cosine dist:0.1895\n",
      "---------------\n",
      " the short answer is : read it back as ordereddict and then delete key(your sheet name ) and save the modified dictionaries to a file .\n",
      "\n",
      "cosine dist:0.1906\n",
      "---------------\n",
      " transform your basket file like this :\n",
      "\n",
      "cosine dist:0.1913\n",
      "---------------\n",
      " \"you should try reading the csv with pandas , the pandas library will automatically parse your csv into columns / rows :\"\n",
      "\n",
      "cosine dist:0.1936\n",
      "---------------\n",
      " let 's put that list into a numpy array :\n",
      "\n",
      "cosine dist:0.1943\n",
      "---------------\n",
      " read your error messages :\n",
      "\n",
      "cosine dist:0.1959\n",
      "---------------\n",
      " dask dataframe only partitions data by rows . see the dask dataframe documentation\n",
      "\n",
      "cosine dist:0.1962\n",
      "---------------\n",
      " for reading the csv file and generate a dict file from it you should definitively have a look at http://docs.python.org/library/csv.html#csv.dictreader .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('Read data into pandas dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.2142\n",
      "---------------\n",
      " there is currently no other way that using a third party app to create a rest api in django . this is something not natively supported in django .\n",
      "\n",
      "cosine dist:0.2367\n",
      "---------------\n",
      " you can initialize a with 0.0flag_val = 0.0 def frozenindex(x ) : if x>5 : current = gen.next ( ) flag_val = current else : current = flag_val return current\n",
      "\n",
      "cosine dist:0.2393\n",
      "---------------\n",
      " use a list comprehension :\n",
      "\n",
      "cosine dist:0.2595\n",
      "---------------\n",
      " \"well i could n't figure out what 's wrong with your code yet . but from the question , i guess this is what you 're looking for\"\n",
      "\n",
      "cosine dist:0.2603\n",
      "---------------\n",
      " \"to replace that loop at the bottom , you could do something like :\"\n",
      "\n",
      "cosine dist:0.2638\n",
      "---------------\n",
      " i think you 're making this harder than it needs to be . either sum them and divide by the number of terms :\n",
      "\n",
      "cosine dist:0.2659\n",
      "---------------\n",
      " \"we can do this in one pass : > > > from collections import defaultdict > > > counter = defaultdict(lambda : defaultdict(int ) ) > > > s = ' the dog chased the cat ' > > > tokens = s.split ( ) > > > from itertools import islice > > > for a , b in zip(tokens , islice(tokens , 1 , none ) ) : ... counter[a][b ] + = 1 ... > > > counter defaultdict(<function < lambda > at 0x102078950 > , { ' the ' : defaultdict(<class ' int ' > , { ' cat ' : 1 , ' dog ' : 1 } ) , ' dog ' : defaultdict(<class ' int ' > , { ' chased ' : 1 } ) , ' chased ' : defaultdict(<class ' int ' > , { ' the ' : 1 } ) } )\"\n",
      "\n",
      "cosine dist:0.2666\n",
      "---------------\n",
      " you can tell read_csv what the character for decimal point is :\n",
      "\n",
      "cosine dist:0.2717\n",
      "---------------\n",
      " \"is it worth pointing out that ( and to a lesser extent , compact ( ) ) is one of the most \"\" evil \"\" features of php ( along with register_globals and eval ) , and should be avoided ?\"\n",
      "\n",
      "cosine dist:0.2726\n",
      "---------------\n",
      " try the babel module . it includes a parser in babel.messages.catalog and babel.messages.pofile among other things .\n",
      "\n",
      "cosine dist:0.2726\n",
      "---------------\n",
      " you can simply make sure it 's in the page source before using any of the methods above .\n",
      "\n",
      "cosine dist:0.2744\n",
      "---------------\n",
      " \"everything is divisible by 1 , so that check will consider everything as non - prime . you need to set the lower bound to check to 2 instead . range(2 , n ) )\"\n",
      "\n",
      "cosine dist:0.2756\n",
      "---------------\n",
      " i think your problem is in this line of code :\n",
      "\n",
      "cosine dist:0.2757\n",
      "---------------\n",
      " \"this may be because the class is not fully initialized when you do your setattr(a , p , v ) there . class a(object ) : pass setattr(a , property , value )\"\n",
      "\n",
      "cosine dist:0.2775\n",
      "---------------\n",
      " \"you should be able to simply let pil get the filetype from extension , i.e. use :\"\n",
      "\n",
      "cosine dist:0.2789\n",
      "---------------\n",
      " to install gspread you need to specify as a package name when using pip : pip install gspread\n",
      "\n",
      "cosine dist:0.2797\n",
      "---------------\n",
      " you 're not passing to foo . it 's possible that working with global variables is slowing your script down .\n",
      "\n",
      "cosine dist:0.2804\n",
      "---------------\n",
      " \"just a guess , but perhaps create your initial icon as an \"\" emptyicon \"\" , then copy the bmp to it .\"\n",
      "\n",
      "cosine dist:0.2809\n",
      "---------------\n",
      " \"if you do n't want to use , you can replace it by .pop(key ) . for example , using unpacking argument too : d = dict(d1 , * * d2 ) d.pop(\"\"country \"\" )\"\n",
      "\n",
      "cosine dist:0.2821\n",
      "---------------\n",
      " \"a limitation ( or \"\" limitation \"\" ) of python closures , comparing to javascript closures , is that it can not be used for effective data hiding\"\n",
      "\n",
      "cosine dist:0.2825\n",
      "---------------\n",
      " using pcre a solution would be : abc(?!.*(abc|xyz).*123).*123(?!.*(abc|xyz).*xyz).*xyz\n",
      "\n",
      "cosine dist:0.2841\n",
      "---------------\n",
      " \"using and calendar.monthrange : > > > from datetime import date , timedelta > > > import calendar > > > start_date = date(1983 , 12 , 23 ) > > > days_in_month = calendar.monthrange(start_date.year , start_date.month)[1 ] > > > start_date + timedelta(days = days_in_month ) datetime.date(1984 , 1 , 23 )\"\n",
      "\n",
      "cosine dist:0.2850\n",
      "---------------\n",
      " \"maybe you could try json module ( note : i have n't used it myself so i do n't know if it 'll solve your problem , it 's just a suggestion ) :\"\n",
      "\n",
      "cosine dist:0.2853\n",
      "---------------\n",
      " \"can you stay under command line ? if yes , try the python lib nammed \"\" pexpect \"\" . it 's pretty useful , and let you run commands like on a terminal , from a python program , and interact with the terminal !\"\n",
      "\n",
      "cosine dist:0.2879\n",
      "---------------\n",
      " \"after 1 year of working with telegram api , i wanted to update this answer for best possible ways to interact with telegram api .\"\n",
      "\n",
      "cosine dist:0.2906\n",
      "---------------\n",
      " \"is the most significant bitdef less_msb(x , y ) : return x < y and x < ( x ^ y )\"\n",
      "\n",
      "cosine dist:0.2922\n",
      "---------------\n",
      " one way you can deal with it is to use a term query ( or term filter ) . this should do it :\n",
      "\n",
      "cosine dist:0.2950\n",
      "---------------\n",
      " you could convert your string to an integer and check it that way :\n",
      "\n",
      "cosine dist:0.2970\n",
      "---------------\n",
      " \"pyqt exposes c++ code to python via sip ; pyside does so via shiboken . both have roughly the same capabilities as swig ( except that they only support \"\" extended c++ to python \"\" , while swig has back - ends for ruby , perl , java , and so forth as well ) . neither swig nor sip and shiboken are designed to interoperate with each other . you could n't conveniently use swig to wrap any code using the c++ extensions that qt requires ( to support signals and slots ) and i have no idea what perils may await you in trying to interoperate sip - wrapped ( or shiboken - wrapped ) and swig - wrapped code .\"\n",
      "\n",
      "cosine dist:0.2988\n",
      "---------------\n",
      " double % chars let you put % 's in format strings .\n",
      "\n",
      "cosine dist:0.3015\n",
      "---------------\n",
      " \"you need look behind syntax ( ? < = ) , which asserts a desired pattern is preceded by another pattern , extract digits after the word marks : followed by optional spaces : s # 0 some texts ... final exam marks:50 next lev ... # 1 some texts .... final exam marks:54 next le ... # 2 some texts ... final marks : 45 next best le ... # name : 1 , dtype : object s.str.extract(\"\"(?<=marks : ) * ( [ 0 - 9]+ ) \"\" , expand = false ) # 0 50 # 1 54 # 2 45 # name : 1 , dtype : object\"\n",
      "\n",
      "cosine dist:0.3020\n",
      "---------------\n",
      " \"i had the same problem too , it seemed that changing the default installation directory for python can cause this problem .\"\n",
      "\n",
      "cosine dist:0.3024\n",
      "---------------\n",
      " to match your last update :\n",
      "\n",
      "cosine dist:0.3034\n",
      "---------------\n",
      " you could do that with a regex :\n",
      "\n",
      "cosine dist:0.3039\n",
      "---------------\n",
      " \"add each from a row to a list , then work through the list setting the state.from pyqt4 import qtcore , qtgui import sys class mainwindow(qtgui . qmainwindow ) : def _ _ init__(self ) : super(mainwindow , self).__init _ _ ( ) self.create_combos ( ) def create_combos(self ) : widget = qtgui . qwidget ( ) self.setcentralwidget(widget ) # create combo boxes and add them to a list . self.combo1 = qtgui . qcombobox ( ) self.combo2 = qtgui . qcombobox ( ) self.combo3 = qtgui . qcombobox ( ) self.combobox_row = [ self.combo1 , self.combo2 , self.combo3 ] # create a toggle button and connect it to the toggle method . self.button = qtgui . qpushbutton('toggle ' ) self.button.setcheckable(true ) self.button.setchecked(true ) self.button.toggled.connect(self.enable_combobox_row ) # create layout . vbox = qtgui . qvboxlayout ( ) vbox.addwidget(self.combo1 ) vbox.addwidget(self.combo2 ) vbox.addwidget(self.combo3 ) vbox.addwidget(self.button ) widget.setlayout(vbox ) def enable_combobox_row(self , enabled ) : # work through combo boxes and set the passed enabled state . for combobox in self.combobox_row : combobox.setenabled(enabled ) if _ _ name _ _ = = ' _ _ main _ _ ' : app = qtgui . qapplication(sys.argv ) mainwindow = mainwindow ( ) mainwindow.show ( ) sys.exit(app.exec_ ( ) )\"\n",
      "\n",
      "cosine dist:0.3043\n",
      "---------------\n",
      " \"i think that using infinite loop is conceptually wrong , because an algorithm is , by its definition , composed by a finite numbers of instructions ( wikipedia ) :\"\n",
      "\n",
      "cosine dist:0.3052\n",
      "---------------\n",
      " \"for the second part of your question , data is typically read from and written to a hard drive in blocks of 512 bytes . so using a block size that is a multiple of that should give the most efficient transfer . other than that , it does n't matter much . just keep in mind that whatever block size you specify is the amount of data that the i / o operation stores in memory at any given time , so do n't choose something so large that it uses up a lot of your ram . i think 8 k ( 8192 ) is a common choice , but 64 k should be fine . ( i do n't think the size of the file being transferred matters much when you 're choosing the best block size )\"\n",
      "\n",
      "cosine dist:0.3055\n",
      "---------------\n",
      " you are spawning a new local child . you should instead be sending the scp command to the ssh_child which is running a shell on the jumpserver .\n",
      "\n",
      "cosine dist:0.3057\n",
      "---------------\n",
      " the problem is this line :\n",
      "\n",
      "cosine dist:0.3059\n",
      "---------------\n",
      " the official python facebook app i know is https://github.com/facebook/runwithfriends if you look through it most if not all about facebook python app is there .\n",
      "\n",
      "cosine dist:0.3079\n",
      "---------------\n",
      " \"tl;dr : with your current program , the in - memory layout of the data should be should be r - g - b - r - g - b - r - g - b - r - g - b ...\"\n",
      "\n",
      "cosine dist:0.3079\n",
      "---------------\n",
      " and typeerror : unorderable types : nonetype ( ) < float ( ) are important hints . you should consider that actual error messages are more important than almost anything else you said about the problems you 're having .\n",
      "\n",
      "cosine dist:0.3079\n",
      "---------------\n",
      " \"if you 're using python2.7 , microsoft recently published a special vc++ compiler for python : http://www.microsoft.com/en-us/download/details.aspx?id=44266\"\n",
      "\n",
      "cosine dist:0.3085\n",
      "---------------\n",
      " \"in server.py , you are using\"\n",
      "\n",
      "cosine dist:0.3086\n",
      "---------------\n",
      " \"add correct namespace and app name for ( see last line)urlpatterns = patterns ( '' , url(r'^activate / complete/$ ' , templateview.as_view(template_name='registration / activation_complete.html ' ) , name='registration_activation_complete ' ) , # activation keys get matched by \\w+ instead of the more specific # [ a - fa - f0 - 9]{40 } because a bad activation key should still get to the view ; # that way it can return a sensible \"\" invalid key \"\" message instead of a # confusing 404 . url(r'^activate/(?p < activation_key>\\w+)/$ ' , activationview.as_view ( ) , name='registration_activate ' ) , url(r'^register/$ ' , customregistrationview.as_view ( ) , name='registration_register ' ) , url(r'^register / complete/$ ' , templateview.as_view(template_name='registration / registration_complete.html ' ) , name='registration_complete ' ) , url(r'^register / closed/$ ' , templateview.as_view(template_name='registration / registration_closed.html ' ) , name='registration_disallowed ' ) , ( r '' , include('registration.auth_urls ' ) , namespace='accounts ' , app_name='registration ' ) , )\"\n",
      "\n",
      "cosine dist:0.3094\n",
      "---------------\n",
      " try changing your render command to this ...\n",
      "\n",
      "cosine dist:0.3095\n",
      "---------------\n",
      " \"one option is to use python 's slicing and indexing features to logically evaluate the places where your condition holds and overwrite the data there.import pandas df = pandas.read_csv(\"\"test.csv \"\" ) df.loc[df.id = = 103 , ' firstname ' ] = \"\" matt \"\" df.loc[df.id = = 103 , ' lastname ' ] = \"\" jones \"\"\"\n",
      "\n",
      "cosine dist:0.3097\n",
      "---------------\n",
      " \"the answer is actually not os - specific -- you can do it within gtk . you can get a list of all the toplevel windows from the application using , then iterate through it until you find one where window.is_active ( ) returns true .\"\n",
      "\n",
      "cosine dist:0.3109\n",
      "---------------\n",
      " \"you can use , also , and catch the error . you can follow this example : def gold_room ( ) : print(\"\"the room is full of gold , how much do you take ? \"\" ) choice = raw_input ( \"\" > \"\") try : return int(choice ) except valueerror : pass print gold_room ( )\"\n",
      "\n",
      "cosine dist:0.3117\n",
      "---------------\n",
      " \"i think some of the confusion arises from a lack of knowledge about parameters . if you look at , the board inside the parentheses is a variable . within display_board(board ) he 's looking at the contents of the provided board , not the one defined in new_board . def display_board(variable ) : \"\" \"\" \"\" display game board on screen . \"\" \"\" \"\" print(\"\"\\n\\t \"\" , variable[0 ] , \"\" | \"\" , variable[1 ] , \"\" | \"\" , variable[2 ] ) print(\"\"\\t \"\" , \"\" --------- \"\" ) print(\"\"\\t \"\" , variable[3 ] , \"\" | \"\" , variable[4 ] , \"\" | \"\" , variable[5 ] ) print(\"\"\\t \"\" , \"\" --------- \"\" ) print(\"\"\\t \"\" , variable[6 ] , \"\" | \"\" , variable[7 ] , \"\" | \"\" , variable[8 ] , \"\" \\n \"\" )\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('train a deep learning model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1614\n",
      "---------------\n",
      " draw keypoints as filled white circles :\n",
      "\n",
      "cosine dist:0.1798\n",
      "---------------\n",
      " try to threshold the input image .\n",
      "\n",
      "cosine dist:0.1899\n",
      "---------------\n",
      " find affine transformation to make rectangle axis - aligned . it is just rotation by angle\n",
      "\n",
      "cosine dist:0.1936\n",
      "---------------\n",
      " just plot with the dataframe values :\n",
      "\n",
      "cosine dist:0.1979\n",
      "---------------\n",
      " you can pick the color in the hsl space\n",
      "\n",
      "cosine dist:0.1986\n",
      "---------------\n",
      " save the bins and use pd.cut again :\n",
      "\n",
      "cosine dist:0.1999\n",
      "---------------\n",
      " provide the overlay alpha mask parameter and see if this yields results you expected :\n",
      "\n",
      "cosine dist:0.2001\n",
      "---------------\n",
      " it sounds like you need to focus on the interpolation of data and then extract values from desired coordinates . for 1d splrep and 2d bisplrep are the interpolation functions you need to check out ( a good overview ) . both of these functions can be weighted and provide fine tune control over the spline function you interpolate with .\n",
      "\n",
      "cosine dist:0.2011\n",
      "---------------\n",
      " draw a line segment between those points :\n",
      "\n",
      "cosine dist:0.2019\n",
      "---------------\n",
      " \"like sshashank124 said , you need to offset the image x , y position with its width , height dimensions . an example being :\"\n",
      "\n",
      "cosine dist:0.2029\n",
      "---------------\n",
      " \"instead of changing the ticks , why not change the units instead ? make a separate array of x - values whose units are in nm . this way , when you plot the data it is already in the correct format ! just make sure you add a xlabel to indicate the units ( which should always be done anyways).from pylab import * # generate random test data in your range n = 200 epsilon = 10**(-9.0 ) x = epsilon*(50*random(n ) + 1 ) y = random(n ) # x2 now has the \"\" units \"\" of nanometers by scaling x x2 = ( 1/epsilon ) * x subplot(121 ) scatter(x , y ) xlim(epsilon,50*epsilon ) xlabel(\"\"meters \"\" ) subplot(122 ) scatter(x2,y ) xlim(1 , 50 ) xlabel(\"\"nanometers \"\" ) show ( )\"\n",
      "\n",
      "cosine dist:0.2030\n",
      "---------------\n",
      " i recommend linking the x - axes together ; this will ensure that their values are aligned with each other regardless of the size of the y - axis . see .ui.graphicsview_y.setxlink(ui.graphicsview_x ) ui.graphicsview_z.setxlink(ui.graphicsview_x )\n",
      "\n",
      "cosine dist:0.2037\n",
      "---------------\n",
      " try getting frames as tutorial suggests . note renaming frame to cap :\n",
      "\n",
      "cosine dist:0.2041\n",
      "---------------\n",
      " creating random sample data :\n",
      "\n",
      "cosine dist:0.2045\n",
      "---------------\n",
      " you can get the point coordinates from a polydata object like so :\n",
      "\n",
      "cosine dist:0.2051\n",
      "---------------\n",
      " \"show horizontal , hide vertical scroll bar :\"\n",
      "\n",
      "cosine dist:0.2053\n",
      "---------------\n",
      " you can get the color cycle from the rc - params .\n",
      "\n",
      "cosine dist:0.2082\n",
      "---------------\n",
      " \"martineau 's comment is on point : in order to draw antialiased filled shapes with pygame , use the module and draw one regular filled shape and one antialiased outline . from http://www.pygame.org/docs/ref/gfxdraw.html :\"\n",
      "\n",
      "cosine dist:0.2086\n",
      "---------------\n",
      " clear the plot in each iteration\n",
      "\n",
      "cosine dist:0.2087\n",
      "---------------\n",
      " \"calculate the 2d position of the point , and use it create the annotation . if you need interactive with the figure , you can recalculate the location when mouse released .\"\n",
      "\n",
      "cosine dist:0.2087\n",
      "---------------\n",
      " it 's not clear if you can set the alpha value for each point separately but if you use multiple plot command it works .\n",
      "\n",
      "cosine dist:0.2088\n",
      "---------------\n",
      " i would guess the image is produced by adding some gaussian function to the grid .\n",
      "\n",
      "cosine dist:0.2091\n",
      "---------------\n",
      " you can solve the problem by calculating the histogram of the image . the below plot shows the peaks of the image .\n",
      "\n",
      "cosine dist:0.2103\n",
      "---------------\n",
      " you could use some heuristic to sort the contours . the following example uses the minimum x coordinate of each contour to sort them :\n",
      "\n",
      "cosine dist:0.2108\n",
      "---------------\n",
      " \"if it is exact copies of the images you want , you can start comparing pixel 1,1 of all images , and group them by what is the same value on pixel 1,1 . after that you know groups ( hopefully quite many groups ? ) and than compare for each group pixel 1,2 . that way you do pixel by pixel untill you get a hundred groups or so . than you just compare them full on , in each group . that way you do your slow n^4 algorithm , but each time on groups of five images , instead of on 500 images at a time . i am assuming you can read in your images pixel by pixel , i know that s possible if they are in .fits , with the pyfits module , but i guess alternatives exist for pretty much any image format ?\"\n",
      "\n",
      "cosine dist:0.2122\n",
      "---------------\n",
      " you can change the font and font - size of individual widgets .\n",
      "\n",
      "cosine dist:0.2124\n",
      "---------------\n",
      " \"@goncalops answer will work if you select the light shapes , but not their transforms .\"\n",
      "\n",
      "cosine dist:0.2125\n",
      "---------------\n",
      " \"if you want to create a fixed size button , you should initialize it , passing width and height .\"\n",
      "\n",
      "cosine dist:0.2130\n",
      "---------------\n",
      " \"if you want to change the color and the marker , you need to plot several scatter plots , at least one for each marker .\"\n",
      "\n",
      "cosine dist:0.2131\n",
      "---------------\n",
      " you can set the node height and width as follows :\n",
      "\n",
      "cosine dist:0.2133\n",
      "---------------\n",
      " you can obtain the screen size using the following :\n",
      "\n",
      "cosine dist:0.2137\n",
      "---------------\n",
      " you could scale the figure\n",
      "\n",
      "cosine dist:0.2141\n",
      "---------------\n",
      " to plot the confusion matrix do the following :\n",
      "\n",
      "cosine dist:0.2142\n",
      "---------------\n",
      " \"if you are sure of the colours of the circle , easier method be to filter the colors using a mask and then apply hough circles as mathew pope suggested .\"\n",
      "\n",
      "cosine dist:0.2143\n",
      "---------------\n",
      " \"just get the bounding box for each contour , use that as a roi to extract the area and save it out :\"\n",
      "\n",
      "cosine dist:0.2146\n",
      "---------------\n",
      " you can try the following solution : leave some overlapping area between two attached images . do linear interpolation between two images in the overlapped area . linear interpolation : h*a + ( 1-h)*b when h goes from 0 to 1 .\n",
      "\n",
      "cosine dist:0.2147\n",
      "---------------\n",
      " here comes an implementation that allow you to set hexagonal cell the color you want and also allow to create custom border colors .\n",
      "\n",
      "cosine dist:0.2152\n",
      "---------------\n",
      " you can perform difference of gaussians . the idea is to blur the image with two different kernels and subtract their respective results :\n",
      "\n",
      "cosine dist:0.2154\n",
      "---------------\n",
      " one way is to check the scale before setting icon :\n",
      "\n",
      "cosine dist:0.2154\n",
      "---------------\n",
      " create the hours array :\n",
      "\n",
      "cosine dist:0.2158\n",
      "---------------\n",
      " you can display a white box around a pie chart plot by turning on the frame .\n",
      "\n",
      "cosine dist:0.2163\n",
      "---------------\n",
      " an idea is make a rectangle png image with semitransparent colors . then use create_image instead of create_rectangle .\n",
      "\n",
      "cosine dist:0.2168\n",
      "---------------\n",
      " you can manually reshape sift output to be suitable for cv2.calcopticalflowpyrlk :\n",
      "\n",
      "cosine dist:0.2179\n",
      "---------------\n",
      " \"as i understand it , your goal is to evaluate the probability density of each image pixels value with respect to a mixture of multivariate normal distributions.def compute_probabilities_faster(img , kernels ) : means , covs , weights = map(np.dstack , zip(*kernels ) ) pixels_as_rows = img.reshape((-1 , 3 , 1 ) ) responses = np.exp(-0.5 * ( ( pixels_as_rows - means ) * * 2 / covs).sum(axis=1 ) ) factors = 1 . / np.sqrt(covs.prod(axis=1 ) * ( ( 2 * np.pi ) * * 3 ) ) return np.sum(responses * factors * weights , axis=2).reshape(img.shape[:2 ] )\"\n",
      "\n",
      "cosine dist:0.2180\n",
      "---------------\n",
      " a example using incrementing area of convex hull can be applied to your case .\n",
      "\n",
      "cosine dist:0.2183\n",
      "---------------\n",
      " you need to rotate the pie labels manually . to this end you may loop over the labels and set the rotation to your needs .\n",
      "\n",
      "cosine dist:0.2185\n",
      "---------------\n",
      " \"spaces the val_arr values out over the default colormap , and the colorbar explains what the val_arr colors mean .\"\n",
      "\n",
      "cosine dist:0.2186\n",
      "---------------\n",
      " \"i tried to modify the contour plots by mixing up the data calculated as a sum along an axis with the grid created by . i calculated the sum of the density along the axis on which i want to have the contour . this looks as follows : # plot projection of density onto z - axis plotdat = np.sum(density , axis=2 ) plotdat = plotdat / np.max(plotdat ) plotx , ploty = np.mgrid[-4:4:100j , -4:4:100j ] ax.contour(plotx , ploty , plotdat , offset=-4 , zdir='z ' ) # this is new # plot projection of density onto y - axis plotdat = np.sum(density , axis=1 ) # summing up density along y - axis plotdat = plotdat / np.max(plotdat ) plotx , plotz = np.mgrid[-4:4:100j , -4:4:100j ] ax.contour(plotx , plotdat , plotz , offset=4 , zdir='y ' ) # plot projection of density onto x - axis plotdat = np.sum(density , axis=0 ) # summing up density along z - axis plotdat = plotdat / np.max(plotdat ) ploty , plotz = np.mgrid[-4:4:100j , -4:4:100j ] ax.contour(plotdat , ploty , plotz , offset=-4 , zdir='x ' ) # continue with your code\"\n",
      "\n",
      "cosine dist:0.2186\n",
      "---------------\n",
      " \"one way is to expand the array to be the same shape as theta and r. this is necessary so that the polar plot extends all the way around ( and matches up at theta=0.import numpy as np import matplotlib.pyplot as plt # populated arrays with angles . azimuths = np.random.random(200)*360 zeniths = np.random.random(200)*180 a_bins = np.linspace(0,360,13 ) z_bins = np.linspace(0,180,7 ) grid , ae , ze = np.histogram2d(azimuths , zeniths , bins=[a_bins , z_bins ] ) a_bins = np.radians(a_bins ) r , theta = np.meshgrid(z_bins , a_bins ) # extend grid by one column row , using the 0th column and row g = np.zeros(r.shape ) g[:-1,:-1 ] = grid g[-1 ] = g[0 ] # copy the top row to the bottom g[:,-1 ] = g[:,0 ] # copy the left column to the right print g.shape,r.shape,theta.shape # # # ( 13 , 7 ) ( 13 , 7 ) ( 13 , 7 ) # plot fig , ax = plt.subplots(subplot_kw=dict(projection='polar ' ) ) cax = ax.contourf(theta , r , g , 30 ) cb = fig.colorbar(cax ) plt.show ( )\"\n",
      "\n",
      "cosine dist:0.2189\n",
      "---------------\n",
      " you can access to axes range from axisitem . here is sample code .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('visualize location distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.2197\n",
      "---------------\n",
      " \"i modified your and it appears to work for me . if you still get an error , please post the full error . > > > import gmpy2 > > > > > > def unpack(x , b ) : ... try : ... return [ x for x in gmpy2.unpack(gmpy2.mpz(x ) , b ) ] ... except nameerror : ... b = 2 * * b ... r = [ ] ... while x : ... x , temp = divmod(x , b ) ... r.append(temp ) ... return r ... > > > unpack(123456**7 , 15 ) [ mpz(0 ) , mpz(0 ) , mpz(4096 ) , mpz(25855 ) , mpz(24508 ) , mpz(31925 ) , mpz(15111 ) , mpz(10775 ) ] > > > del(gmpy2 ) > > > unpack(123456**7 , 15 ) [ 0 , 0 , 4096 , 25855 , 24508 , 31925 , 15111 , 10775 ]\"\n",
      "\n",
      "cosine dist:0.2203\n",
      "---------------\n",
      " every python object has a special member called which is a dictionary containing all the instance 's member .\n",
      "\n",
      "cosine dist:0.2217\n",
      "---------------\n",
      " \"this is similar to @user3238855 's answer , but i 'm using python 3 's function , with a filter ( ) instead of a comprehension , and not saving a reference to a list of lines.with open('info.txt ' , ' r ' ) as f : print(*filter(str.istitle , f ) , sep='\\n ' )\"\n",
      "\n",
      "cosine dist:0.2313\n",
      "---------------\n",
      " \"i think you can use addition , because of column date is datetime and column heure is timedelta : df.index = df . date + df . heure print ( df ) numerom date heure reg 2016 - 05 - 26 15:32:00 2396 2016 - 05 - 26 15:32:00 2396 2016 - 05 - 26 15:34:00 2397 2016 - 05 - 26 15:34:00 75599 2016 - 05 - 26 15:35:00 2398 2016 - 05 - 26 15:35:00 5822 2016 - 05 - 26 15:37:00 2399 2016 - 05 - 26 15:37:00 45225 2016 - 05 - 26 15:38:00 2400 2016 - 05 - 26 15:38:00 5266\"\n",
      "\n",
      "cosine dist:0.2408\n",
      "---------------\n",
      " \"if you have some data that you want to be available in all templates throughout your site , you could create a \"\" context_processor \"\" . this is simply a function that returns a dictionary . that dictionary is added to the context ( and therefore available in your template ) when rendering any view with a . def get_searchdata ( ) : spec_list = speciality.objects.order_by('name ' ) ins_list = insurance.objects.order_by('name ' ) dict_data = { ' specialties ' : spec_list , ' insurances ' : ins_list } return dict_data\"\n",
      "\n",
      "cosine dist:0.2477\n",
      "---------------\n",
      " \"have you installed the package in your datalab environment , or on your local machine ? you 'll need to run the following command within datalab:!pip install google - cloud - storage\"\n",
      "\n",
      "cosine dist:0.2494\n",
      "---------------\n",
      " here 's a shorter and simpler way of solving your problem .\n",
      "\n",
      "cosine dist:0.2522\n",
      "---------------\n",
      " \"if no _ _ cmp _ _ ( ) , _ _ eq _ _ ( ) or _ _ ne _ _ ( ) operation is defined , class instances are compared by object identity ( “ address ” ) .\"\n",
      "\n",
      "cosine dist:0.2541\n",
      "---------------\n",
      " \"the reason for the apparent inefficiency of the k - d tree is quite simple : you are measuring both the construction and querying of the k - d tree at once . this is not how you would or should use a k - d tree : you should construct it only once . if you measure only the querying , the time taken reduces to mere tens of milliseconds ( vs seconds using the brute - force approach ) .\"\n",
      "\n",
      "cosine dist:0.2557\n",
      "---------------\n",
      " you can simply do this .\n",
      "\n",
      "cosine dist:0.2561\n",
      "---------------\n",
      " \"you can use for column kiwis and then use apply with selecting columns by subset [ ] : import matplotlib.pyplot as plt def violin(col ) : sns.violinplot(y=col , x=\"\"kiwis \"\" , data = df , split = true , inner=\"\"quart \"\" ) plt.figure ( ) cols = df.columns.difference(['kiwis ' ] ) df[cols].apply(violin )\"\n",
      "\n",
      "cosine dist:0.2590\n",
      "---------------\n",
      " \"you can use for a much cleaner recursive function : class tree : def _ _ init__(self , * * kwargs ) : self.__dict _ _ = { i : kwargs.get(i ) for i in [ ' val ' , ' _ left ' , ' _ right ' ] } def count_nodes(self ) : yield 1 yield from getattr(self._left , ' count_nodes ' , lambda : [ ] ) ( ) yield from getattr(self._right , ' count_nodes ' , lambda : [ ] ) ( ) t = tree(_left = tree(val = 12 ) , val = 37 ) result = sum(t.count_nodes ( ) )\"\n",
      "\n",
      "cosine dist:0.2610\n",
      "---------------\n",
      " \"after much back - and - forth , this code should do what you want , barring any bugs :) . to anyone else ; there may be a few more changes made to iron out any kinks .\"\n",
      "\n",
      "cosine dist:0.2633\n",
      "---------------\n",
      " \"returns a list , so you 'll want to access the first ( and only , i 'd assume ) element.fig = figure(x_axis_type=\"\"datetime \"\" , height=200 , tools=\"\"tap \"\" ) fig.line(x_range , y_range ) news_points = fig.circle(x='timestamp ' , y='y ' , fill_color=\"\"green \"\" , size=5 , source = news_source ) url = \"\" @url \"\" taptool = fig.select(type=taptool)[0 ] taptool.renderers.append(news_points ) taptool.callback = openurl(url = url )\"\n",
      "\n",
      "cosine dist:0.2635\n",
      "---------------\n",
      " in this line :\n",
      "\n",
      "cosine dist:0.2635\n",
      "---------------\n",
      " \"the essential problem is that is a class , where you expect it to be a function . the code inside it runs exactly once , when the whole class definition is parsed , rather than each time you call the class as you seem to expect .\"\n",
      "\n",
      "cosine dist:0.2638\n",
      "---------------\n",
      " from documentation for zipfile module\n",
      "\n",
      "cosine dist:0.2647\n",
      "---------------\n",
      " matplotlib has a interface which works very similar : http://matplotlib.org/users/path_tutorial.htmlimport matplotlib.path as mpath import matplotlib.patches as patches import matplotlib.pyplot as plt\n",
      "\n",
      "cosine dist:0.2657\n",
      "---------------\n",
      " so i believe the below will work\n",
      "\n",
      "cosine dist:0.2658\n",
      "---------------\n",
      " \"not sure that my sample is a good style , but you can use something like this :\"\n",
      "\n",
      "cosine dist:0.2666\n",
      "---------------\n",
      " why not utilize str.rfind ?\n",
      "\n",
      "cosine dist:0.2696\n",
      "---------------\n",
      " \"your code is extremely inefficient because you the second file inside the loop iterating over the first file . just read the second file into a list ( or better yet , a set which gives you on average o(1 ) lookup time ) and use the in operator . also , your linecnt variable just counts the number of lines in file1 - you can just read the lines into a list and call len on this list to get the same number : def compfiles(file1 , file2 ) : lines1 = [ l.strip ( ) for l in open(file1).read().split(\"\"\\n \"\" ) ] lines2 = set([l.strip ( ) for l in open(file2).read().split(\"\"\\n \"\" ) ] ) for line in lines1 : if not line in lines2 : print(\"\"miss : file % s contains ' % s ' , but file % s does not ! \"\" % ( file1 , line , file2 ) ) print(\"\"%i lines compared between % s and % s. \"\" % ( len(lines1 ) , file1 , file2 ) )\"\n",
      "\n",
      "cosine dist:0.2711\n",
      "---------------\n",
      " \"the problem here is that there are tasks queued which use the old method signature , being that without the timestamp variable . without some more information , it is hard to determine your best course of action .\"\n",
      "\n",
      "cosine dist:0.2719\n",
      "---------------\n",
      " append does n't return an object . so you assigned none to the variable free .\n",
      "\n",
      "cosine dist:0.2724\n",
      "---------------\n",
      " \"just create a path beforehand , via : request_number = 82673 # base dir _ dir = \"\" d:\\current download \"\" # create dynamic name , like \"\" d:\\current download\\attachment82673 \"\" _ dir = os.path.join(_dir , ' attachment%s ' % request_number ) # create ' dynamic ' dir , if it does not exist if not os.path.exists(_dir ) : os.makedirs(_dir )\"\n",
      "\n",
      "cosine dist:0.2729\n",
      "---------------\n",
      " \"you might want to try using the function from the itertools recipes.from itertools import filterfalse def unique_everseen(iterable , key = none ) : \"\" list unique elements , preserving order . remember all elements ever seen . \"\" # unique_everseen('aaaabbbccdaabbb ' ) -- > a b c d # unique_everseen('abbccad ' , str.lower ) -- > a b c d seen = set ( ) seen_add = seen.add if key is none : for element in filterfalse(seen.__contains _ _ , iterable ) : seen_add(element ) yield element else : for element in iterable : k = key(element ) if k not in seen : seen_add(k ) yield element thing = [ [ 20,0,1],[20,0,2],[20,1,1],[30,1,1 ] ] thing.sort(key=lambda x : 0 if x[1 ] = = 1 else 1 ) print(list(unique_everseen(thing , key = lambda x : ( x[0 ] , x[2 ] ) ) ) )\"\n",
      "\n",
      "cosine dist:0.2730\n",
      "---------------\n",
      " \"if you insist on not using : open high low close volume date 2017 - 11 - 01 44.66 44.75 42.19 42.93 3500 2017 - 11 - 03 44.66 44.75 42.19 42.93 3500 2017 - 11 - 06 43.15 43.75 40.60 41.02 9200 2017 - 11 - 07 43.15 43.75 40.60 41.02 9200 2017 - 11 - 08 43.15 43.75 40.60 41.02 9200 2017 - 11 - 09 43.15 43.75 40.60 41.02 9200 2017 - 11 - 10 43.15 43.75 40.60 41.02 9200 2017 - 11 - 13 41.60 43.21 40.03 42.36 3575 2017 - 11 - 14 41.60 43.21 40.03 42.36 3575 ... df.corr ( ) # open high low close volume # open 1.000000 0.891708 0.957078 0.351604 0.320314 # high 0.891708 1.000000 0.878307 0.610183 0.311939 # low 0.957078 0.878307 1.000000 0.559366 0.146151 # close 0.351604 0.610183 0.559366 1.000000 -0.132609 # volume 0.320314 0.311939 0.146151 -0.132609 1.000000 np.corrcoef(df.loc[:,'open':].values.t ) # array ( [ [ 1 . , 0.89170836 , 0.95707833 , 0.35160354 , 0.32031362 ] , # [ 0.89170836 , 1 . , 0.87830748 , 0.61018322 , 0.31193906 ] , # [ 0.95707833 , 0.87830748 , 1 . , 0.55936625 , 0.14615072 ] , # [ 0.35160354 , 0.61018322 , 0.55936625 , 1 . , -0.13260909 ] , # [ 0.32031362 , 0.31193906 , 0.14615072 , -0.13260909 , 1 . ] ] )\"\n",
      "\n",
      "cosine dist:0.2733\n",
      "---------------\n",
      " logging module is by default there in python 3 environment .no need to import it .\n",
      "\n",
      "cosine dist:0.2772\n",
      "---------------\n",
      " have you tried using instead of argparse ? you can do something like that then : import sys dict = { } tmp = [ ] key = '' for arg in sys.argv : if arg[0 ] = = ' - ' : if tmp ! = [ ] : dict[key ] = tmp tmp = [ ] key = arg if key = = ' --args ' : dict[key ] = sys.argv[sys.argv.find(key)+1 : ] break continue tmp.append(arg )\n",
      "\n",
      "cosine dist:0.2774\n",
      "---------------\n",
      " you want to use ' append ' mode for your write .\n",
      "\n",
      "cosine dist:0.2776\n",
      "---------------\n",
      " \"import requests import datetime import pandas as pd def daily_price_historical(symbol , comparison_symbol , limit=1 , aggregate=1 , exchange= '' , alldata='true ' ) : url = ' https://min - api.cryptocompare.com / data / histoday?fsym={}&tsym={}&limit={}&aggregate={}&alldata={}'\\ .format(symbol.upper ( ) , comparison_symbol.upper ( ) , limit , aggregate , alldata ) if exchange : url + = ' & e={}'.format(exchange ) page = requests.get(url ) data = page.json()['data ' ] df = pd . dataframe(data ) df['timestamp ' ] = [ datetime.datetime.fromtimestamp(d ) for d in df.time ] # i do n't have the following function , but it 's not needed to run this # datetime.datetime.fromtimestamp ( ) return df df = daily_price_historical('btc ' , ' eth ' ) print(df )\"\n",
      "\n",
      "cosine dist:0.2777\n",
      "---------------\n",
      " \"use : new_lista = [ j for i , j in enumerate(lista ) if i not in listb ]\"\n",
      "\n",
      "cosine dist:0.2780\n",
      "---------------\n",
      " do n't reinvent the wheel .. use django - reversion for logging changes .\n",
      "\n",
      "cosine dist:0.2781\n",
      "---------------\n",
      " please refer to the example for details .\n",
      "\n",
      "cosine dist:0.2782\n",
      "---------------\n",
      " \"one option would be to create a twin axes using , and then you can control the yticks on the two axes separately . import matplotlib.pyplot as plt import numpy as np # fake up some data spread = np.random.rand(50 ) * 100 center = np.ones(25 ) * 50 flier_high = np.random.rand(10 ) * 100 + 100 flier_low = np.random.rand(10 ) * -100 data = np.concatenate((spread , center , flier_high , flier_low ) , 0 ) # make the figure and axes fig , ax = plt.subplots(1 ) # add you twin axes ax2 = ax.twinx ( ) # set the ticks to the outside on the right only ax2.tick_params(axis='y',direction='out ' ) # make sure the ticks and axes limits are shared between the left and right ax.get_shared_y_axes().join(ax,ax2 ) # basic boxplot ax.boxplot(data ) plt.show ( )\"\n",
      "\n",
      "cosine dist:0.2782\n",
      "---------------\n",
      " this is how my path dir looks like : home / brice / google_projects / google_appengine\n",
      "\n",
      "cosine dist:0.2784\n",
      "---------------\n",
      " \"will allow you to set the atime and mtime of an existing filesystem object , defaulting to the current date and time.os.utime(path )\"\n",
      "\n",
      "cosine dist:0.2787\n",
      "---------------\n",
      " define : return an iterator that keeps a reference to the current node . then it can get the next node in o(1 ) . _ _ getitem _ _ traverses to the requested node in o(n ) .\n",
      "\n",
      "cosine dist:0.2787\n",
      "---------------\n",
      " \"you need to add python to your path . i could be wrong , but windows 7 should have the same cmd as windows 8 . try this in the command line . using permanently makes changes to you path . note there are no equal signs , and quotes are used.setx path \"\" % pythonpath%;c:\\python27 \"\"\"\n",
      "\n",
      "cosine dist:0.2787\n",
      "---------------\n",
      " this indeed drives one mad . however here are some things :\n",
      "\n",
      "cosine dist:0.2790\n",
      "---------------\n",
      " \"on the rest api , your are allowed 180 queries every 15 minutes , and i guess the streaming api has a similar limitation . you do not want to come too close to this limit , since your application will eventually get blocked even if you do not strictly hit it . since your problem has something to do with the rate limit , you should put a sleep in your loop . i 'd say a sleep(4 ) should be enough , but it 's mostly a matter of trial and error there , try to change the value and see for yourself.sleeptime = 4 pages = tweepy . cursor(api.followers , screen_name=\"\"username\"\").pages ( ) while true : try : page = next(pages ) time.sleep(sleeptime ) except tweepy . tweeperror : # taking extra care of the \"\" rate limit exceeded \"\" time.sleep(60*15 ) page = next(pages ) except stopiteration : break for user in page : print(user.id_str ) print(user.screen_name ) print(user.followers_count )\"\n",
      "\n",
      "cosine dist:0.2800\n",
      "---------------\n",
      " \"to add text to the beginning of a file , you can ( 1 ) open the file for reading , ( 2 ) read the file , ( 3 ) open the file for writing and overwrite it with ( your text + the original file text ) .\"\n",
      "\n",
      "cosine dist:0.2807\n",
      "---------------\n",
      " your logic should be enough . if you have say :\n",
      "\n",
      "cosine dist:0.2834\n",
      "---------------\n",
      " \"as of bokeh ( and soon to be 0.12 ) there is no mechanism to hide the map portion . it seems like a reasonable feature though , an probably not terribly difficult to implement . i encourage you to submit a feature request on the project 's github issue tracker .\"\n",
      "\n",
      "cosine dist:0.2837\n",
      "---------------\n",
      " \"you 'll need to define the spark schema explicitly and pass it to the createdataframe function : from pyspark.sql.types import * import pandas as pd small = pdf.read_csv(\"\"data.csv \"\" ) small.head ( ) # mycolumns # 0 nan # 1 a sch = structtype([structfield(\"\"mycolumns \"\" , stringtype ( ) , true ) ] ) df = spark.createdataframe(small , sch ) df.show ( ) # + ---------+ # |mycolumns| # + ---------+ # | nan| # | a| # + ---------+ df.printschema ( ) # root # |-- mycolumns : string ( nullable = true )\"\n",
      "\n",
      "cosine dist:0.2838\n",
      "---------------\n",
      " \"parentheses denote that you actually want to call the method , rather than mention it for later use . on a line of its own , the latter does n't appear to make sense , but in larger expressions , it allows delaying the function call :\"\n",
      "\n",
      "cosine dist:0.2845\n",
      "---------------\n",
      " no the macros do n't become gpl derived works anymore than a ' c ' program written on a gpl linux system or with a gpl gcc compiler become gpl .\n",
      "\n",
      "cosine dist:0.2845\n",
      "---------------\n",
      " \"up to me , you need to write custom code . there is no builtin function in python to do what you want to achieve .\"\n",
      "\n",
      "cosine dist:0.2847\n",
      "---------------\n",
      " \"alerts are annoying show an error message instead , but you need to pass in context to check later if you need to display the error message or not : return render('action.html ' , { ' no_record_check ' : no_record_check } )\"\n",
      "\n",
      "cosine dist:0.2853\n",
      "---------------\n",
      " \"maybe python is throwing an error and not printing to stdout because of that error . did you forget to import sleep from the time module , or where does it come from ?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('start a web server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1530\n",
      "---------------\n",
      " you should send get request with cookies :\n",
      "\n",
      "cosine dist:0.1683\n",
      "---------------\n",
      " try to send csrf token\n",
      "\n",
      "cosine dist:0.1858\n",
      "---------------\n",
      " send a multipart email with the appropriate mime types .\n",
      "\n",
      "cosine dist:0.1993\n",
      "---------------\n",
      " get the username :\n",
      "\n",
      "cosine dist:0.2006\n",
      "---------------\n",
      " you have to send the and connection : close headers .\n",
      "\n",
      "cosine dist:0.2054\n",
      "---------------\n",
      " you need to send a valid http request . for example :\n",
      "\n",
      "cosine dist:0.2067\n",
      "---------------\n",
      " \"you need to send the output string ,\"\n",
      "\n",
      "cosine dist:0.2074\n",
      "---------------\n",
      " you should generate e - mail in html format and insert needed tags\n",
      "\n",
      "cosine dist:0.2097\n",
      "---------------\n",
      " to handle cookies you should use a session that stores cookies between requests :\n",
      "\n",
      "cosine dist:0.2110\n",
      "---------------\n",
      " you can pass username and password as arguments :\n",
      "\n",
      "cosine dist:0.2121\n",
      "---------------\n",
      " you need to send json with additional header entries :\n",
      "\n",
      "cosine dist:0.2141\n",
      "---------------\n",
      " \"since you are sending the http requests from another domain , you need to make sure that your is able to send cookies . in your app 's config , add:$httpprovider.defaults.withcredentials = true\"\n",
      "\n",
      "cosine dist:0.2156\n",
      "---------------\n",
      " pass the server name as a parameter :\n",
      "\n",
      "cosine dist:0.2171\n",
      "---------------\n",
      " you need to loop the code on the client side that accepts the input and sends it .\n",
      "\n",
      "cosine dist:0.2179\n",
      "---------------\n",
      " you are supposed to get authorization token using received code . this token will be used to access deviantart afterwards .\n",
      "\n",
      "cosine dist:0.2190\n",
      "---------------\n",
      " use if you want flask to accept any host name .\n",
      "\n",
      "cosine dist:0.2201\n",
      "---------------\n",
      " alternatively send the same header that you use with the curl command ...\n",
      "\n",
      "cosine dist:0.2222\n",
      "---------------\n",
      " the important part is to send the message in the required format :\n",
      "\n",
      "cosine dist:0.2234\n",
      "---------------\n",
      " you can not do that because the browser sends just the /details.html for request .\n",
      "\n",
      "cosine dist:0.2244\n",
      "---------------\n",
      " \"you need to send a header back the the browser ; all you do is create cookie data but you are not sending it back.print \"\" content - type : text / html \"\"\"\n",
      "\n",
      "cosine dist:0.2249\n",
      "---------------\n",
      " pass the port separately from the host :\n",
      "\n",
      "cosine dist:0.2256\n",
      "---------------\n",
      " you 're comparing what you are seeing with a browser with what requests generates ( i.e. there is no user agent header ) . if you specify this before making the initial request it will reflect what you would see in a web browser . google serves the requests differently it looks like :\n",
      "\n",
      "cosine dist:0.2265\n",
      "---------------\n",
      " the problem is that your code is sending username and password as post data instead of using the proper http authorization header .\n",
      "\n",
      "cosine dist:0.2265\n",
      "---------------\n",
      " use requests.session to obtain a session by logging in .\n",
      "\n",
      "cosine dist:0.2274\n",
      "---------------\n",
      " \"you need to post to the login page , save the cookie , then send the cookies along with other subsequent requests . something like this :\"\n",
      "\n",
      "cosine dist:0.2277\n",
      "---------------\n",
      " attach it as a header :\n",
      "\n",
      "cosine dist:0.2291\n",
      "---------------\n",
      " \"the easiest method would be to send back some sort of return code and when the client sees the return code from the server , it would throw the exception itself .\"\n",
      "\n",
      "cosine dist:0.2293\n",
      "---------------\n",
      " you need send this to server . i guess you ajax for your example .\n",
      "\n",
      "cosine dist:0.2296\n",
      "---------------\n",
      " you need to pass a http header or check the http referral when you get a request to . ( in a controlled / secure environment aka intranet )\n",
      "\n",
      "cosine dist:0.2299\n",
      "---------------\n",
      " your server is sending down post request and you are fetching a get call .\n",
      "\n",
      "cosine dist:0.2303\n",
      "---------------\n",
      " you need to add the credentials for your email ( username and password ) like this :\n",
      "\n",
      "cosine dist:0.2303\n",
      "---------------\n",
      " you can make secure the api calls by passing the access - token with every get or post request . the access - token can be send either through query - params or with the http_authorisation_header.also use tokenauthentication for verifying the access - token validity . for setting tokenauthentication as default for all api calls add the given line to settings file :\n",
      "\n",
      "cosine dist:0.2311\n",
      "---------------\n",
      " if the web server supports the range request then you can add the range header to your request :\n",
      "\n",
      "cosine dist:0.2325\n",
      "---------------\n",
      " \"your server is expecting a higher sequence number . to fix this problem , you can send resetseqnumflag ( tag = 141 ) as y as after this your server should also reset its sequence number .\"\n",
      "\n",
      "cosine dist:0.2331\n",
      "---------------\n",
      " just fetch the related article and pass that into template ...\n",
      "\n",
      "cosine dist:0.2332\n",
      "---------------\n",
      " \"follow this link to get your client id , secret and callback uri . then make sure you set those parameters correctly in call\"\n",
      "\n",
      "cosine dist:0.2339\n",
      "---------------\n",
      " \"it always comes down to the request / response model . you just have to craft a series of http requests such that you get the desired responses . in this case , you also need the server to treat each request as part of the same session . to do that , you need to figure out how the server is tracking sessions . it could be a number of things , from cookies to hidden inputs to form actions , post data , or query strings . if i had to guess i 'd put my money on a cookie in this case ( i have n't checked the links ) . if this holds true , you need to send the first request , save the cookie you get back , and then send that cookie along with the 2nd request .\"\n",
      "\n",
      "cosine dist:0.2342\n",
      "---------------\n",
      " \"you need to send a new line character ( ) as if you were using telnet yourself.tn.write(\"\"admin\\n \"\" )\"\n",
      "\n",
      "cosine dist:0.2343\n",
      "---------------\n",
      " \"you should be able to log in now . if not , try with ' user - agent ' and ' referer ' in the headers\"\n",
      "\n",
      "cosine dist:0.2344\n",
      "---------------\n",
      " \"with such restrictions as not having zero client side data , you could pass a session token in the get parameters of every link rendered in the html page .\"\n",
      "\n",
      "cosine dist:0.2346\n",
      "---------------\n",
      " here 's how you send and receive json through django .\n",
      "\n",
      "cosine dist:0.2351\n",
      "---------------\n",
      " \"so , you 'll need to send the language code in config in your request\"\n",
      "\n",
      "cosine dist:0.2352\n",
      "---------------\n",
      " you expect to receive twice as much data as you send .\n",
      "\n",
      "cosine dist:0.2353\n",
      "---------------\n",
      " change the logging config to send logs to file :\n",
      "\n",
      "cosine dist:0.2361\n",
      "---------------\n",
      " \"thomas , you should always use correlationid of a response to find which request it is associated with . iirc , reference data and historical data requests support multiple securities , but market data and intraday bars may have only one security per request .\"\n",
      "\n",
      "cosine dist:0.2364\n",
      "---------------\n",
      " \"if python is sending out the email through your smtp server ; you 'll want to change the email type to html formatting by setting the content - type to # build the email message sender_name = \"\" my script \"\" sender_email = \"\" someemail@company.com \"\" reciver_emails = [ ' receive1@company.com ' , ' receive2@company.com ' ] subject = \"\" my email subject \"\" message = \"\" html < b > bolded</b > text \"\" email = ( \"\" from : % s < % s>\\r\\n \"\" \"\" to : % s\\r\\n \"\" % ( sender_name , sender_email , receiver_emails ) ) email = email + \"\" cc : % s\\r\\n \"\" % ( cc_emails ) email = email + ( \"\" mime - version : 1.0\\r\\n \"\" \"\" content - type : text / html\\r\\n \"\" \"\" subject : % s\\r\\n\\r\\n \"\" \"\" \"\" \"\" < html>\\r\\n \"\" \"\" % s\\r\\n \"\" \"\" < /html > \"\" % ( subject , message ) )\"\n",
      "\n",
      "cosine dist:0.2384\n",
      "---------------\n",
      " \"you should use the header in the request . but you may use it only if the server informs you that it accept range request by accept - ranges response header . > head /2238/2758537173_670161cac7_b.jpg http/1.1 > host : farm3.static.flickr.com > accept : * / * > < http/1.1 200 ok < date : thu , 08 jul 2010 12:22:12 gmt < content - type : image / jpeg < connection : keep - alive < server : apache/2.0.52 ( red hat ) < expires : mon , 28 jul 2014 23:30:00 gmt < last - modified : we d , 13 aug 2008 06:13:54 gmt < accept - ranges : bytes < content - length : 350015\"\n",
      "\n",
      "cosine dist:0.2387\n",
      "---------------\n",
      " \"your ajax is sending the data as \"\" username \"\" but your python is looking for \"\" search \"\" .\"\n",
      "\n",
      "cosine dist:0.2388\n",
      "---------------\n",
      " \"you should use the head request for this , it asks the webserver for the headers without the body . see how do you send a head http request in python 2 ?\"\n",
      "\n",
      "cosine dist:0.2390\n",
      "---------------\n",
      " translate host to ip :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('send out email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine dist:0.1476\n",
      "---------------\n",
      " parse from file :\n",
      "\n",
      "cosine dist:0.1544\n",
      "---------------\n",
      " \"parse the html using beautifulsoup , then only retrieve the text .\"\n",
      "\n",
      "cosine dist:0.1860\n",
      "---------------\n",
      " you can use beautifulsoup for parsing the html string .\n",
      "\n",
      "cosine dist:0.1887\n",
      "---------------\n",
      " \"you can parse the json , then output it again with indents like this :\"\n",
      "\n",
      "cosine dist:0.1947\n",
      "---------------\n",
      " \"use json . generate the css dynamically , using caching to reduce load .\"\n",
      "\n",
      "cosine dist:0.1996\n",
      "---------------\n",
      " \"use lxml.html , it handles invalid xhtml better .\"\n",
      "\n",
      "cosine dist:0.2003\n",
      "---------------\n",
      " load the data first :\n",
      "\n",
      "cosine dist:0.2005\n",
      "---------------\n",
      " this can be done by generating html in python to replace the html of a text area .\n",
      "\n",
      "cosine dist:0.2011\n",
      "---------------\n",
      " \"first , unescape html entities , then remove punctuation chars :\"\n",
      "\n",
      "cosine dist:0.2025\n",
      "---------------\n",
      " simply sanitize your input :\n",
      "\n",
      "cosine dist:0.2033\n",
      "---------------\n",
      " you ca n't just parse the pdf with a regex to extract the text . in most cases the text in inside compressed binary blobs or encoded . a pdf with the text shown like this is very much the exception .\n",
      "\n",
      "cosine dist:0.2051\n",
      "---------------\n",
      " \"you 'll first need to parse the html content for img src urls with something like lxml or beautifulsoup . then , you can feed one of those img src urls into sorl - thumbnail or easy - thumbnails as edmon suggests .\"\n",
      "\n",
      "cosine dist:0.2062\n",
      "---------------\n",
      " \"encode it as json , mark it safe , and print it out .\"\n",
      "\n",
      "cosine dist:0.2063\n",
      "---------------\n",
      " you could use beautifulsoup to parse xml :\n",
      "\n",
      "cosine dist:0.2065\n",
      "---------------\n",
      " \"if you are generating the html as text , you can use string formatting :\"\n",
      "\n",
      "cosine dist:0.2070\n",
      "---------------\n",
      " of course you can construct the text of your query as you want using variables . e.g.\n",
      "\n",
      "cosine dist:0.2086\n",
      "---------------\n",
      " \"all you need to do is , parse the img tag to find the url and download it using something like urllib.urlretrieve .\"\n",
      "\n",
      "cosine dist:0.2094\n",
      "---------------\n",
      " use beautifulsoup parser for parsing html files .\n",
      "\n",
      "cosine dist:0.2101\n",
      "---------------\n",
      " use beautifulsoup or lxml to parse the html .\n",
      "\n",
      "cosine dist:0.2103\n",
      "---------------\n",
      " try bs4 you can parse a html document by\n",
      "\n",
      "cosine dist:0.2118\n",
      "---------------\n",
      " you could format your query before execute it like this :\n",
      "\n",
      "cosine dist:0.2145\n",
      "---------------\n",
      " \"first , i 'd parse the format file .\"\n",
      "\n",
      "cosine dist:0.2162\n",
      "---------------\n",
      " transform your basket file like this :\n",
      "\n",
      "cosine dist:0.2174\n",
      "---------------\n",
      " you can grab the urls using regex like :\n",
      "\n",
      "cosine dist:0.2175\n",
      "---------------\n",
      " \"i would recommend checking out beautifulsoup for parsing the returned page . with it , you can loop through the links and extract the link address fairly easy and append them to a list of the links .\"\n",
      "\n",
      "cosine dist:0.2190\n",
      "---------------\n",
      " encode it while printing\n",
      "\n",
      "cosine dist:0.2203\n",
      "---------------\n",
      " you can use beautifulsoup to extract just text and not modify the tags . its in their documentation .\n",
      "\n",
      "cosine dist:0.2211\n",
      "---------------\n",
      " \"simple , parse mp3 binary blob to calculate something , in python\"\n",
      "\n",
      "cosine dist:0.2217\n",
      "---------------\n",
      " extract the archive and execute :\n",
      "\n",
      "cosine dist:0.2218\n",
      "---------------\n",
      " \"you could use the module , along with xpath . lxml is good for parsing xml / html , traversing element trees and returning element text / attributes . you can select particular elements , sets of elements or attributes of elements using xpath . using your example data : content = ' '' < track > < trackpoint > < time>2015 - 08 - 29t22:04:39.000z</time > < position > < latitudedegrees>37.198049426078796</latitudedegrees > < longitudedegrees>127.07204628735781</longitudedegrees > < /position > < altitudemeters>34.79999923706055</altitudemeters > < distancemeters>7.309999942779541</distancemeters > < heartratebpm > < value>102</value > < /heartratebpm > < cadence>76</cadence > < extensions > < tpx xmlns=\"\"http://www.garmin.com / xmlschemas / activityextension / v2 \"\" > < watts>112</watts > < /tpx > < /extensions > < /trackpoint > .... lots of < trackpoint > ... < /trackpoint > < /track > ' '' from lxml import etree tree = etree . xml(content ) time = tree.xpath('trackpoint/time/text ( ) ' ) print(time )\"\n",
      "\n",
      "cosine dist:0.2224\n",
      "---------------\n",
      " \"you could generate html first , then automatize pandoc to convert it into an odt . then , using the pyuno bridge , convert it to a doc . it 's a pretty long chain but i did not figure out a better way of doing it . also have a look at http://wiki.services.openoffice.org/wiki/python .\"\n",
      "\n",
      "cosine dist:0.2236\n",
      "---------------\n",
      " terminology : parsers do n't write xml ; they read xml . serialisers write xml .\n",
      "\n",
      "cosine dist:0.2240\n",
      "---------------\n",
      " the following is one of the ways to extract data from xml file using xml.etree library and python 2.7 . replace the xml file name and the tag name in the corresponding places in the code .\n",
      "\n",
      "cosine dist:0.2252\n",
      "---------------\n",
      " \"if your goal is showing actual data , you can render page as usual ( without loop ) and reload page with js :\"\n",
      "\n",
      "cosine dist:0.2260\n",
      "---------------\n",
      " you can reverse admin list page url as follow :\n",
      "\n",
      "cosine dist:0.2267\n",
      "---------------\n",
      " you must use beautifulsoup while dealing with html or xml files .\n",
      "\n",
      "cosine dist:0.2269\n",
      "---------------\n",
      " \"the problem is not really your regex , but the fact that beautifulsoup parse the html ( its job after all ) and change its content . for example , your will be transformed to < br/>. another point : soup.text erases all the tags , so your regex wo n't work anymore.from bs4 import * import re from dateutil import parser pattern = re.compile(r'sent:(.+?)(?=<br/ > ) ' ) with open(\"\"myfile.html \"\" , ' r ' ) as f : html = f.read ( ) print(\"\"html : \"\" , html ) soup = beautifulsoup(html , ' lxml ' ) print(\"\"soup.text : \"\" , soup.text ) print(\"\"str(soup ) : \"\" , str(soup ) ) a = pattern.findall(str(soup))[0 ] print(\"\"pattern extraction : \"\" , a )\"\n",
      "\n",
      "cosine dist:0.2278\n",
      "---------------\n",
      " \"simply convert your doc files to docx . you can use this html parsing python library , beautiful soup .\"\n",
      "\n",
      "cosine dist:0.2278\n",
      "---------------\n",
      " you can capture the output and print it from python itself . say :\n",
      "\n",
      "cosine dist:0.2287\n",
      "---------------\n",
      " just decode from json again :\n",
      "\n",
      "cosine dist:0.2288\n",
      "---------------\n",
      " \"use an xml parser ( i suggest lxml ) to load your xml , and to create new dom trees .\"\n",
      "\n",
      "cosine dist:0.2301\n",
      "---------------\n",
      " \"avoid parsing html with regex , use an html parser instead .\"\n",
      "\n",
      "cosine dist:0.2303\n",
      "---------------\n",
      " using this code you can get url of first array\n",
      "\n",
      "cosine dist:0.2305\n",
      "---------------\n",
      " \"elaborating more on what jean - fracois fabre mentioned , can be used to read a json file and parse into python object representation of json . json.loads ( ) does the same except that the input is a string instead of a file ( see json module for more details).import json file = open('logs.txt ' ) data = json.load(file ) # now the json object is represented as python dict for key in data.keys ( ) : # dev - server and uat - server are keys service_status = data[key]['servicestatus ' ] # this would give out the servicestatus host_status = data[key]['hoststatus ' ] # this would give out the hoststatus\"\n",
      "\n",
      "cosine dist:0.2311\n",
      "---------------\n",
      " \"if i got you right , you want to remove certain tags ( and eventually their contents ) from your xml file . try using lxml for processing the lxml file . have a look at these functions from lxml.etree .\"\n",
      "\n",
      "cosine dist:0.2314\n",
      "---------------\n",
      " you can tokenize the file and use that to print function definitions :\n",
      "\n",
      "cosine dist:0.2315\n",
      "---------------\n",
      " the linebreaks template tag is used to render newlines ( from text fields ) into html breaks .\n",
      "\n",
      "cosine dist:0.2316\n",
      "---------------\n",
      " remove r from view code .\n",
      "\n",
      "cosine dist:0.2316\n",
      "---------------\n",
      " please see this response : regular expressions to parse template tags in xml\n",
      "\n",
      "cosine dist:0.2322\n",
      "---------------\n",
      " \"usually modifying the bs4 parse tree is unnecessary . you can just get the div 's text , if that 's what you wanted :\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "se.search('parse html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Embeddings (Optional)\n",
    "\n",
    "We highly recommend using [tensorboard](https://www.tensorflow.org/versions/r1.0/get_started/embedding_viz) as way to visualize embeddings.  Tensorboard contains an interactive search that makes it easy (and fun) to explore embeddings.  We leave this as an exercise to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
